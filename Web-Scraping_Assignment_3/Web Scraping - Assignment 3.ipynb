{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc4f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "\n",
    "# importing the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebebb0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in ./opt/anaconda3/lib/python3.9/site-packages (4.1.3)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in ./opt/anaconda3/lib/python3.9/site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in ./opt/anaconda3/lib/python3.9/site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in ./opt/anaconda3/lib/python3.9/site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: sortedcontainers in ./opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in ./opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in ./opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: sniffio in ./opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in ./opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: idna in ./opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in ./opt/anaconda3/lib/python3.9/site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in ./opt/anaconda3/lib/python3.9/site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in ./opt/anaconda3/lib/python3.9/site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in ./opt/anaconda3/lib/python3.9/site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in ./opt/anaconda3/lib/python3.9/site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: cffi>=1.12 in ./opt/anaconda3/lib/python3.9/site-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.14.6)\n",
      "Requirement already satisfied: pycparser in ./opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.20)\n",
      "Requirement already satisfied: six>=1.5.2 in ./opt/anaconda3/lib/python3.9/site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in ./opt/anaconda3/lib/python3.9/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install selenium\n",
    "\n",
    "# Import all the required libraries\n",
    "import selenium                   # Library that is use to work with selenium\n",
    "import pandas as pd               # To Create DataFrame\n",
    "from selenium import webdriver    # Importing webdriver module from selenium to open up automated chrome window\n",
    "import warnings                   # To Ignore any sort of warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time                       # Use to stop search engine for few seconds\n",
    "import csv                        # Use to Read/Write data in CSV File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41b949ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb98eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcfdbea",
   "metadata": {},
   "source": [
    "### Q 1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProductDetails(search_Item):\n",
    "    # First connect to the web driver\n",
    "    driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "    # Open amazon.com website on automated chrome window\n",
    "    driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "    # Finding element for Product Search Bar and Entering product Laptop\n",
    "    search_field_product = driver.find_element_by_xpath(\"//input[@class='nav-input nav-progressive-attribute'][@type='text']\")  # Product Search Bar\n",
    "    search_field_product.send_keys(search_Item)\n",
    "    \n",
    "    # Clicking on Search Button\n",
    "    search_button = driver.find_element_by_xpath(\"//input[@class='nav-input nav-progressive-attribute'][@type='submit']\")\n",
    "    search_button.click()\n",
    "\n",
    "    time.sleep(2)\n",
    "    return driver\n",
    "\n",
    "    \n",
    "search_Item = input(\"Kindly enter the product to be searched : \")\n",
    "getProductDetails(search_Item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca157b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff132c7",
   "metadata": {},
   "source": [
    "### Q 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e659da",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_Item = input(\"Kindly enter the product to be searched : \")\n",
    "\n",
    "result_driver = getProductDetails(search_Item)\n",
    "\n",
    "product_names = []\n",
    "product_links = []\n",
    "\n",
    "for i in range(0, 3):          # Running for loop with range to run this loop 3 times\n",
    "\n",
    "#     product_results = result_driver.find_elements_by_xpath(\"//div[@data-component-type='s-search-result']\")\n",
    "#     for product in product_results:\n",
    "#         product_names.append(product.find_element_by_xpath(\"//span[@class='a-size-base-plus a-color-base a-text-normal']\").text())\n",
    "#         product_links.append(product.find_element_by_xpath(\"//a[@class='a-link-normal s-no-outline']\").get_attribute('href'))\n",
    "\n",
    "#         try:\n",
    "#             product.click()\n",
    "#             time.sleep(1)\n",
    "#         except Exception:\n",
    "#             continue\n",
    "\n",
    "#         # extract product details\n",
    "#         product_brand = result_driver.find_elements_by_xpath(\"//img[@class='n3VNCb KAlRDb']\")\n",
    "# #         for actual_image in actual_images:\n",
    "# #             if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
    "# #                 \n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Clicking on Next Button\n",
    "    if(i < 2):\n",
    "        result_driver.find_element_by_xpath(\"//a[@class='s-pagination-item s-pagination-next s-pagination-button s-pagination-separator']\").click()  # Locating web element of next button and then clicking it\n",
    "    time.sleep(5)              # Using time to pause the search engine for 5 second\n",
    "\n",
    "# Close automated chrome window\n",
    "result_driver.close()\n",
    "\n",
    "# Display all data in 1 table\n",
    "products = pd.DataFrame({'Product Name': product_names, 'Link': product_links})\n",
    "products.index += 1                                # Start Index from 1\n",
    "products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a9b2b",
   "metadata": {},
   "source": [
    "### Q 3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb4606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImgDetails(search_Item):\n",
    "    \n",
    "    # First connect to the web driver\n",
    "    driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Open images.google.com website on automated chrome window\n",
    "    driver.get(\"https://images.google.com/\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Finding element for Image Search Bar and Entering Item to be searched\n",
    "    search_field_product = driver.find_element_by_xpath(\"//input[@class='gLFyf gsfi']\")  # Image Search Bar\n",
    "    search_field_product.send_keys(search_Item)\n",
    "    \n",
    "    # Clicking on Search Button\n",
    "    search_button = driver.find_element_by_xpath(\"//button[@class='Tg7LZd'][@type='submit']\")\n",
    "    search_button.click()\n",
    "    \n",
    "    image_name = []\n",
    "    image_urls = []\n",
    "    thumbnail_results = driver.find_elements_by_xpath(\"//img[@class='rg_i Q4LuWd']\")\n",
    "    for img in thumbnail_results[0:10]:\n",
    "        try:\n",
    "            img.click()\n",
    "            time.sleep(1)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # extract image urls\n",
    "        actual_images = driver.find_elements_by_xpath(\"//img[@class='n3VNCb KAlRDb']\")\n",
    "        for actual_image in actual_images:\n",
    "            if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
    "                image_urls.append(actual_image.get_attribute('src'))\n",
    "                image_name.append(search_Item)\n",
    "\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Display all data in 1 table\n",
    "    products = pd.DataFrame({'Image': image_name, 'Image Path': image_urls})\n",
    "    products.index += 1                                # Start Index from 1\n",
    "    print(products)\n",
    "    \n",
    "    # Close automated chrome window\n",
    "    driver.close()\n",
    "\n",
    "\n",
    "getImgDetails(\"fruits\")\n",
    "getImgDetails(\"cars\")\n",
    "getImgDetails(\"Machine Learning\")\n",
    "getImgDetails(\"Guitar\")\n",
    "getImgDetails(\"Cakes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a4dc39",
   "metadata": {},
   "source": [
    "### Q 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174222d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException     # Importing exceptions\n",
    "\n",
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open flipkart.com website on automated chrome window\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "# Close Login Pop-up\n",
    "driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "# Finding element for Product Search Bar and Entering product Sunglasses\n",
    "search_field_product = driver.find_element_by_class_name(\"_3704LK\")\n",
    "search_field_product.send_keys(\"Realme Narzo\")\n",
    "\n",
    "# Clicking on Search Button\n",
    "search_button = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_button.click()\n",
    "\n",
    "# Creates a new .csv file that the data will be written to\n",
    "csv_file = open('//Users//nehamishra//MyPython//SmartPhoneList.csv', 'w', encoding=\"UTF-8\", newline=\"\")\n",
    "writer = csv.writer(csv_file)\n",
    "# write header names\n",
    "writer.writerow(['Brand Name', 'Smartphone Name', 'Colour', 'RAM', 'Storage(ROM)', 'Primary Camera', 'Secondary Camera', 'Display Size', 'Battery Capacity', 'Price', 'Product URL'])\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "brand_list = []                   # Brand\n",
    "name_list = []                    # Name\n",
    "colour_list = []                  # Color\n",
    "ram_list = []                     # RAM\n",
    "storage_list = []                 # Storage (ROM)\n",
    "camera_primary_list = []          # Primary Camera\n",
    "camera_secondary_list = []        # Secondary Camera\n",
    "display_size_list = []            # Display Size\n",
    "battery_list = []                 # Battery Capacity\n",
    "price_list = []                   # Price\n",
    "link_list = []                    # Product URL\n",
    "\n",
    "# Extract comment details\n",
    "smartphone_dict = {}\n",
    "\n",
    "search_results = driver.find_elements_by_xpath(\"//a[@class='_1fQZEK']\")\n",
    "\n",
    "for smartphone in search_results:\n",
    "    link_list.append(smartphone.get_attribute('href'))\n",
    "\n",
    "for smartphone_link in link_list:\n",
    "    \n",
    "    driver.get(smartphone_link)\n",
    "    driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _1FH0tX']\").click()\n",
    "    \n",
    "    specification_tables = driver.find_elements_by_xpath(\"//table[@class='_14cfVK']\")\n",
    "    specification_rows = driver.find_elements_by_xpath(\"//tr[@class='_1s_Smc row']\")\n",
    "    \n",
    "    try:\n",
    "        price_list.append(driver.find_element_by_xpath(\"//div[@class='_30jeq3 _16Jk6d']\").text)\n",
    "    except NoSuchElementException:\n",
    "        price_list.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        temp = driver.find_element_by_xpath(\"//tr[@class='_1s_Smc row']//td[@class='_1hKmbr col col-3-12'][text()='User Interface']\")\n",
    "    except NoSuchElementException:\n",
    "        brand_list.append(\"-\")\n",
    "\n",
    "    try:\n",
    "        temp = driver.find_element_by_xpath(\"//tr[@class='_1s_Smc row']//td[@class='_1hKmbr col col-3-12'][text()='Model Name']\")\n",
    "    except NoSuchElementException:\n",
    "        name_list.append(\"-\")\n",
    "\n",
    "    try:\n",
    "        temp = driver.find_element_by_xpath(\"//tr[@class='_1s_Smc row']//td[@class='_1hKmbr col col-3-12'][text()='Color']\")\n",
    "    except NoSuchElementException:\n",
    "        colour_list.append(\"-\")\n",
    "\n",
    "    try:\n",
    "        temp = driver.find_element_by_xpath(\"//tr[@class='_1s_Smc row']//td[@class='_1hKmbr col col-3-12'][text()='RAM']\")\n",
    "    except NoSuchElementException:\n",
    "        ram_list.append(\"-\")\n",
    "\n",
    "    try:\n",
    "        temp = driver.find_element_by_xpath(\"//tr[@class='_1s_Smc row']//td[@class='_1hKmbr col col-3-12'][text()='Internal Storage']\")\n",
    "    except NoSuchElementException:\n",
    "        storage_list.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        temp = driver.find_element_by_xpath(\"//tr[@class='_1s_Smc row']//td[@class='_1hKmbr col col-3-12'][text()='Primary Camera']\")\n",
    "    except NoSuchElementException:\n",
    "        camera_primary_list.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        temp = driver.find_element_by_xpath(\"//tr[@class='_1s_Smc row']//td[@class='_1hKmbr col col-3-12'][text()='Secondary Camera']\")\n",
    "    except NoSuchElementException:\n",
    "        camera_secondary_list.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        temp = driver.find_element_by_xpath(\"//tr[@class='_1s_Smc row']//td[@class='_1hKmbr col col-3-12'][text()='Display Size']\")\n",
    "    except NoSuchElementException:\n",
    "        display_size_list.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        temp = driver.find_element_by_xpath(\"//tr[@class='_1s_Smc row']//td[@class='_1hKmbr col col-3-12'][text()='Battery Capacity']\")\n",
    "    except NoSuchElementException:\n",
    "        battery_list.append(\"-\")\n",
    "        \n",
    "    for row in specification_rows:\n",
    "        \n",
    "        if row.find_element_by_xpath(\".//td[@class='_1hKmbr col col-3-12']\").text == \"User Interface\":\n",
    "            brand_list.append(row.find_element_by_xpath(\".//li[@class='_21lJbe']\").text[0:6])\n",
    "            \n",
    "        if row.find_element_by_xpath(\".//td[@class='_1hKmbr col col-3-12']\").text == \"Model Name\":\n",
    "            name_list.append(row.find_element_by_xpath(\".//li[@class='_21lJbe']\").text)\n",
    "            \n",
    "        if row.find_element_by_xpath(\".//td[@class='_1hKmbr col col-3-12']\").text == \"Color\":\n",
    "            colour_list.append(row.find_element_by_xpath(\".//li[@class='_21lJbe']\").text)\n",
    "        \n",
    "        if row.find_element_by_xpath(\".//td[@class='_1hKmbr col col-3-12']\").text == \"RAM\":\n",
    "            ram_list.append(row.find_element_by_xpath(\".//li[@class='_21lJbe']\").text)\n",
    "        \n",
    "        if row.find_element_by_xpath(\".//td[@class='_1hKmbr col col-3-12']\").text == \"Internal Storage\":\n",
    "            storage_list.append(row.find_element_by_xpath(\".//li[@class='_21lJbe']\").text)\n",
    "        \n",
    "        if row.find_element_by_xpath(\".//td[@class='_1hKmbr col col-3-12']\").text == \"Primary Camera\":\n",
    "            camera_primary_list.append(row.find_element_by_xpath(\".//li[@class='_21lJbe']\").text)\n",
    "        \n",
    "        if row.find_element_by_xpath(\".//td[@class='_1hKmbr col col-3-12']\").text == \"Secondary Camera\":\n",
    "            camera_secondary_list.append(row.find_element_by_xpath(\".//li[@class='_21lJbe']\").text)\n",
    "        \n",
    "        if row.find_element_by_xpath(\".//td[@class='_1hKmbr col col-3-12']\").text == \"Display Size\":\n",
    "            display_size_list.append(row.find_element_by_xpath(\".//li[@class='_21lJbe']\").text)\n",
    "        \n",
    "        if row.find_element_by_xpath(\".//td[@class='_1hKmbr col col-3-12']\").text == \"Battery Capacity\":\n",
    "            battery_list.append(row.find_element_by_xpath(\".//li[@class='_21lJbe']\").text)\n",
    "            \n",
    "# Display all data in 1 table\n",
    "products = pd.DataFrame({'Brand Name': brand_list,'Smartphone Name': name_list, 'Colour': colour_list, 'RAM': ram_list, 'Storage (ROM)': storage_list, 'Primary Camera': camera_primary_list, 'Secondary Camera': camera_secondary_list, 'Display Size': display_size_list, 'Battery Capacity': battery_list, 'Price': price_list, 'Product URL': link_list})\n",
    "products.index += 1                                # Start Index from 1\n",
    "print(products)\n",
    "\n",
    "# Close automated chrome window\n",
    "driver.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5222da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af4484a",
   "metadata": {},
   "source": [
    "### Q 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c767c4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL :  https://www.google.com/maps/place/Jaipur,+Rajasthan/@26.8854479,75.6504701,11z/data=!3m1!4b1!4m5!3m4!1s0x396c4adf4c57e281:0xce1c63a0cf22e09!8m2!3d26.9124336!4d75.7872709\n",
      "geoCode :  []\n",
      "Latitude :  []\n",
      "Longitude :  []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "#search_Location = input(\"Enter Location to be searched : \")\n",
    "\n",
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "driver.maximize_window()\n",
    "\n",
    "# Open maps.google.com website on automated chrome window\n",
    "driver.get(\"https://www.google.com/maps\")\n",
    "time.sleep(1)\n",
    "\n",
    "# Finding element for Map Search Bar and Entering Location to be searched\n",
    "search_field_product = driver.find_element_by_xpath(\"//input[@id='searchboxinput']\")  # Map Search Bar\n",
    "search_field_product.send_keys(\"Jaipur\")\n",
    "\n",
    "# Clicking on Search Button\n",
    "search_button = driver.find_element_by_xpath(\"//button[@id='searchbox-searchbutton']\")\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#location = geolocator.geocode(search_Location)\n",
    "# gn = geocoders.GeoNames()\n",
    "# print(gn.geocode(\"Cleveland, OH 44106\"))\n",
    "\n",
    "# print(\"Latitude : \", location.latitude)\n",
    "# print(\"Longitude : \", location.longitude)\n",
    "\n",
    "# Getting current URL\n",
    "current_url = driver.current_url\n",
    "\n",
    "# Close automated chrome window\n",
    "driver.close()\n",
    "\n",
    "# @\\d{1,3}\\.\\d{5,},-\\d{1,3}\\.\\d{5,}\n",
    "# Latitude: (?<=@)\\d{1,3}\\.\\d{5,}\n",
    "# Longitude: (?<=,)-\\d{1,3}\\.\\d{5,}\n",
    "\n",
    "geoCode = re.findall(r'@/d{1,3}/./d{5,},-/d{1,3}/./d{5,}',current_url)\n",
    "#Latitude = float(re.findall(\"/@(-?[/d/.]*)\", str(geoCode))[0])\n",
    "\n",
    "Latitude = re.findall(\"/@(-?[\\d\\.]*)\",str(geoCode))\n",
    "Longitude = re.findall(\"/@[-?\\d\\.]*/,([-?\\d\\.]*)\",str(geoCode))\n",
    "\n",
    "print(\"URL : \", current_url)\n",
    "print(\"geoCode : \", geoCode)\n",
    "print(\"Latitude : \", Latitude)\n",
    "print(\"Longitude : \", Longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0afe472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace7741",
   "metadata": {},
   "source": [
    "### Q 6. Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dad234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "driver.maximize_window()\n",
    "\n",
    "# Open trak.in website on automated chrome window\n",
    "driver.get(\"https://trak.in/\")\n",
    "\n",
    "# Finding element for Funding Deals and clicking it\n",
    "driver.find_element_by_xpath(\"//a[@title='http://trak.in/india-startup-funding-investment-2015/']\").click()  # Funding Deals\n",
    "time.sleep(1)\n",
    "\n",
    "list_dates = []\n",
    "list_StartupName = []\n",
    "list_IndustryVertical = []\n",
    "list_SubVertical = []\n",
    "list_Location = []\n",
    "list_InvestorName = []\n",
    "list_InvestmentType = []\n",
    "list_Amount = []\n",
    "\n",
    "# January Funding Deals\n",
    "deals_January_table = driver.find_element_by_xpath(\"//table[@id='tablepress-54']\")\n",
    "deals_January = deals_January_table.find_elements_by_xpath(\".//tbody[@class='row-hover']//tr\")\n",
    "print(\"deals_January : \", len(deals_January))\n",
    "for deal in deals_January:\n",
    "    \n",
    "    try:\n",
    "        list_dates.append(deal.find_element_by_xpath(\".//td[@class='column-2']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_dates.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        list_StartupName.append(deal.find_element_by_xpath(\".//td[@class='column-3']//a\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_StartupName.append(\"-\")\n",
    "\n",
    "    try:\n",
    "        list_IndustryVertical.append(deal.find_element_by_xpath(\".//td[@class='column-4']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_IndustryVertical.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        list_SubVertical.append(deal.find_element_by_xpath(\".//td[@class='column-5']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_SubVertical.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        list_Location.append(deal.find_element_by_xpath(\".//td[@class='column-6']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_Location.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        list_InvestorName.append(deal.find_element_by_xpath(\".//td[@class='column-7']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_InvestorName.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        list_InvestmentType.append(deal.find_element_by_xpath(\".//td[@class='column-8']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_InvestmentType.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        list_Amount.append(deal.find_element_by_xpath(\".//td[@class='column-9']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_Amount.append(\"-\")\n",
    "    \n",
    "# February Funding Deals\n",
    "deals_February_table = driver.find_element_by_xpath(\"//table[@id='tablepress-55']\")\n",
    "deals_February = deals_February_table.find_elements_by_xpath(\".//tbody[@class='row-hover']//tr\")\n",
    "print(\"deals_February : \", len(deals_February))\n",
    "for deal in deals_February:\n",
    "    \n",
    "    try:\n",
    "        list_dates.append(deal.find_element_by_xpath(\".//td[@class='column-2']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_dates.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        list_StartupName.append(deal.find_element_by_xpath(\".//td[@class='column-3']//a\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_StartupName.append(\"-\")\n",
    "\n",
    "    try:\n",
    "        list_IndustryVertical.append(deal.find_element_by_xpath(\".//td[@class='column-4']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_IndustryVertical.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        list_SubVertical.append(deal.find_element_by_xpath(\".//td[@class='column-5']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_SubVertical.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        list_Location.append(deal.find_element_by_xpath(\".//td[@class='column-6']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_Location.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        list_InvestorName.append(deal.find_element_by_xpath(\".//td[@class='column-7']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_InvestorName.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        list_InvestmentType.append(deal.find_element_by_xpath(\".//td[@class='column-8']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_InvestmentType.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        list_Amount.append(deal.find_element_by_xpath(\".//td[@class='column-9']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_Amount.append(\"-\")\n",
    "\n",
    "# Maarch Funding Deals\n",
    "deals_March_table = driver.find_element_by_xpath(\"//table[@id='tablepress-56']\")\n",
    "deals_March = deals_March_table.find_elements_by_xpath(\".//tbody[@class='row-hover']//tr\")\n",
    "print(\"deals_March : \", len(deals_March))\n",
    "for deal in deals_March:\n",
    "    \n",
    "    try:\n",
    "        list_dates.append(deal.find_element_by_xpath(\".//td[@class='column-2']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_dates.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        list_StartupName.append(deal.find_element_by_xpath(\".//td[@class='column-3']//a\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_StartupName.append(\"-\")\n",
    "\n",
    "    try:\n",
    "        list_IndustryVertical.append(deal.find_element_by_xpath(\".//td[@class='column-4']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_IndustryVertical.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        list_SubVertical.append(deal.find_element_by_xpath(\".//td[@class='column-5']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_SubVertical.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        list_Location.append(deal.find_element_by_xpath(\".//td[@class='column-6']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_Location.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        list_InvestorName.append(deal.find_element_by_xpath(\".//td[@class='column-7']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_InvestorName.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        list_InvestmentType.append(deal.find_element_by_xpath(\".//td[@class='column-8']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_InvestmentType.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        list_Amount.append(deal.find_element_by_xpath(\".//td[@class='column-9']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_Amount.append(\"-\")\n",
    "\n",
    "# Display all data in 1 table\n",
    "funding_Deals = pd.DataFrame({'Date': list_dates,'Startup Name': list_StartupName, 'Industry Vertical': list_IndustryVertical, 'Sub Vertical': list_SubVertical, 'City/Location': list_Location, 'Investor Name': list_InvestorName, 'Investment Type': list_InvestmentType, 'Amount': list_Amount})\n",
    "funding_Deals.index += 1                                # Start Index from 1\n",
    "print(funding_Deals)\n",
    "\n",
    "# Close automated chrome window\n",
    "driver.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd29dd",
   "metadata": {},
   "source": [
    "### Q 7. Write a program to scrap all the available details of best gaming laptops from digit.in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c9ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "driver.maximize_window()\n",
    "\n",
    "# Open digit.in website on automated chrome window for Best Gaming Laptops \n",
    "driver.get(\"https://www.digit.in/top-products/best-gaming-laptops-40.html\")\n",
    "\n",
    "list_LaptopName = []\n",
    "list_Processor = []\n",
    "list_Display = []\n",
    "list_OS = []\n",
    "list_Memory = []\n",
    "list_GraphicsProcessor = []\n",
    "list_Body = []\n",
    "list_Seller = []\n",
    "list_Price = []\n",
    "\n",
    "list_Laptops = driver.find_elements_by_xpath(\"//table[@id='summtable']//tbody//tr\")\n",
    "for laptop in list_Laptops:\n",
    "    \n",
    "    try:\n",
    "        list_LaptopName.append(laptop.find_element_by_xpath(\".//td[1]\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_LaptopName.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        list_Seller.append(laptop.find_element_by_xpath(\".//td[2]\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_Seller.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        list_Price.append(laptop.find_element_by_xpath(\".//td[3]\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_Price.append(\"-\")\n",
    "        \n",
    "list_Laptop_Details = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//table//tbody//tr\") \n",
    "\n",
    "for laptop_Specs in list_Laptop_Details:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        if((len(laptop_Specs.text) > 0) and (laptop_Specs.text != \"SPECIFICATION\")):\n",
    "            try:\n",
    "                \n",
    "                try:\n",
    "                    \n",
    "                    if(laptop_Specs.find_element_by_xpath(\".//td[1]\").text == \"Processor\"):\n",
    "                        list_Processor.append(laptop_Specs.find_element_by_xpath(\".//td[3]\").text)\n",
    "                        \n",
    "                    if(laptop_Specs.find_element_by_xpath(\".//td[1]\").text == \"Display\"):\n",
    "                        list_Display.append(laptop_Specs.find_element_by_xpath(\".//td[3]\").text)\n",
    "                        \n",
    "                    if(laptop_Specs.find_element_by_xpath(\".//td[1]\").text == \"OS\"):\n",
    "                        list_OS.append(laptop_Specs.find_element_by_xpath(\".//td[3]\").text)\n",
    "                        \n",
    "                    if(laptop_Specs.find_element_by_xpath(\".//td[1]\").text == \"Memory\"):\n",
    "                        list_Memory.append(laptop_Specs.find_element_by_xpath(\".//td[3]\").text)\n",
    "                        \n",
    "                    if(laptop_Specs.find_element_by_xpath(\".//td[1]\").text == \"Graphics Processor\"):\n",
    "                        list_GraphicsProcessor.append(laptop_Specs.find_element_by_xpath(\".//td[3]\").text)\n",
    "                        \n",
    "                    if(laptop_Specs.find_element_by_xpath(\".//td[1]\").text == \"Body\"):\n",
    "                        list_Body.append(laptop_Specs.find_element_by_xpath(\".//td[3]\").text)\n",
    "                        \n",
    "                except NoSuchElementException:\n",
    "                    print(\"No\")\n",
    "\n",
    "            except NoSuchElementException as e:\n",
    "                print(\"N\")\n",
    "        \n",
    "    except NoSuchElementException as e:\n",
    "        print(\"No\")\n",
    "        \n",
    "# Display all data in 1 table\n",
    "laptop_Details = pd.DataFrame({'Laptop': list_LaptopName, 'Seller': list_Seller, 'Price': list_Price, 'Processor': list_Processor, 'Display': list_Display, 'OS': list_OS, 'Memory': list_Memory, 'Graphics Processor': list_GraphicsProcessor, \"Body\": list_Body})\n",
    "laptop_Details.index += 1                                # Start Index from 1\n",
    "print(laptop_Details)\n",
    "\n",
    "# Close automated chrome window\n",
    "driver.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1479da5",
   "metadata": {},
   "source": [
    "### Q 8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d31c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "driver.maximize_window()\n",
    "\n",
    "# Open forbes.com website on automated chrome window for Billionaires List\n",
    "driver.get(\"https://www.forbes.com/billionaires/\")\n",
    "\n",
    "list_Billionaires = []\n",
    "list_Rank = []\n",
    "list_Name = []\n",
    "list_NetWorth = []\n",
    "list_Age = []\n",
    "list_Citizenship = []\n",
    "list_Source = []\n",
    "list_Industry = []\n",
    "\n",
    "# Collapse Detail of No. 1 Billionaire\n",
    "time.sleep(1)\n",
    "driver.find_element_by_xpath(\"//span[@class='expand-row__icon']\").click()\n",
    "\n",
    "list_Billionaires = driver.find_elements_by_xpath(\"//div[@class='table-row ']\")\n",
    "\n",
    "for billionaire in list_Billionaires:\n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            list_Rank.append(billionaire.find_element_by_xpath(\".//div[@class='rank']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Rank.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Name.append(billionaire.find_element_by_xpath(\".//div[@class='personName']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Name.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_NetWorth.append(billionaire.find_element_by_xpath(\".//div[@class='netWorth']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_NetWorth.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Age.append(billionaire.find_element_by_xpath(\".//div[@class='age']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Age.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Citizenship.append(billionaire.find_element_by_xpath(\".//div[@class='countryOfCitizenship']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Citizenship.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Source.append(billionaire.find_element_by_xpath(\".//div[@class='source']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Source.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Industry.append(billionaire.find_element_by_xpath(\".//div[@class='category']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Industry.append(\"-\")\n",
    "\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        print(\"No\")\n",
    "\n",
    "# Display all data in 1 table\n",
    "billionaire_list = pd.DataFrame({'Rank': list_Rank, 'Name': list_Name, 'Net Worth': list_NetWorth, 'Age': list_Age, 'Citizenship': list_Citizenship, 'Source': list_Source, 'Industry': list_Industry})\n",
    "billionaire_list.index += 1                                # Start Index from 1\n",
    "print(billionaire_list)\n",
    "\n",
    "# Close automated chrome window\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75360c6f",
   "metadata": {},
   "source": [
    "### Q 9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c348d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "# Creates a new .csv file that the data will be written to\n",
    "csv_file = open('//Users//nehamishra//MyPython//YoutubeCommentList.csv', 'w', encoding=\"UTF-8\", newline=\"\")\n",
    "writer = csv.writer(csv_file)\n",
    "# write header names\n",
    "writer.writerow(['Comment', 'Comment Upvotes', 'Time'])\n",
    "\n",
    "# Open youtube.com website on automated chrome window\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "driver.get(\"https://www.youtube.com/watch?v=Y1xs_xPb46M\")\n",
    "time.sleep(5)\n",
    "\n",
    "try:\n",
    "        \n",
    "#   Extract comment details\n",
    "    youtube_dict = {}\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        #scroll down to load more comments\n",
    "        driver.execute_script('window.scrollTo(0,390);')\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Loads 20 comments , so scroll 25 times to load next set of 480 comments. \n",
    "        for i in range(0, 25):\n",
    "            driver.execute_script(\"window.scrollTo(0,Math.max(document.documentElement.scrollHeight,document.body.scrollHeight,document.documentElement.clientHeight))\")\n",
    "            time.sleep(5)\n",
    "\n",
    "        #count total number of comments and set index to number of comments if less than 500 otherwise set as 500. \n",
    "        totalcomments= len(driver.find_elements_by_xpath(\"\"\"//*[@id=\"content-text\"]\"\"\"))\n",
    "\n",
    "        if totalcomments < 500:\n",
    "            index= totalcomments\n",
    "        else:\n",
    "            index= 500 \n",
    "\n",
    "        ccount = 0\n",
    "        while ccount < index: \n",
    "            \n",
    "            try:\n",
    "                comment = driver.find_elements_by_xpath('//*[@id=\"content-text\"]')[ccount].text\n",
    "            except:\n",
    "                comment = \"\"\n",
    "            \n",
    "            try:\n",
    "                comment_posted = driver.find_elements_by_xpath('//*[@class=\"published-time-text above-comment style-scope ytd-comment-renderer\"]/a')[ccount].text\n",
    "            except:\n",
    "                comment_posted = \"\"\n",
    "                \n",
    "            try:\n",
    "                upvotes = driver.find_elements_by_xpath('//*[@id=\"vote-count-middle\"]')[ccount].text\n",
    "            except:\n",
    "                upvotes = \"\"\n",
    "\n",
    "            youtube_dict['comment'] = comment\n",
    "            youtube_dict['upvotes'] = upvotes\n",
    "            youtube_dict['comment_posted'] = comment_posted\n",
    "\n",
    "            writer.writerow(youtube_dict.values())\n",
    "            ccount = ccount +1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Exception 1 : \", e)\n",
    "        driver.close()\n",
    "    \n",
    "    print(\"Scrapping process Completed\")\n",
    "    csv_file.close()    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Exception 2 : \", e)\n",
    "    driver.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f290eb",
   "metadata": {},
   "source": [
    "### Q 10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b5c80352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Hostel Name  \\\n",
      "1                  St Christopher's Village   \n",
      "2             Palmers Lodge - Swiss Cottage   \n",
      "3                          Generator London   \n",
      "4                      Urbany Hostel London   \n",
      "5         Safestay London Elephant & Castle   \n",
      "6                        London Backpackers   \n",
      "7   Safestay London Kensington Holland Park   \n",
      "8                St Christopher's Greenwich   \n",
      "9           St Christopher's Shepherds Bush   \n",
      "10                            Selina Camden   \n",
      "11                  Saint James Backpackers   \n",
      "12                      Britannia Inn Hotel   \n",
      "13                              Barry House   \n",
      "14                       London House Hotel   \n",
      "15                            Elmwood Hotel   \n",
      "16                      The W14 Hotel & Bar   \n",
      "17                          Cranbrook Hotel   \n",
      "18                   TLK Apartments & Hotel   \n",
      "19                               Park Hotel   \n",
      "20   Best Western Boltons London Kensington   \n",
      "\n",
      "                                      Distance        Total Reviews  \\\n",
      "1              Hostel - 1.8km from city centre  11227 Total Reviews   \n",
      "2              Hostel - 6.5km from city centre  15357 Total Reviews   \n",
      "3                Hostel - 3km from city centre   6959 Total Reviews   \n",
      "4              Hostel - 5.4km from city centre    317 Total Reviews   \n",
      "5              Hostel - 1.7km from city centre   4266 Total Reviews   \n",
      "6             Hostel - 11.9km from city centre   4246 Total Reviews   \n",
      "7              Hostel - 5.9km from city centre   1202 Total Reviews   \n",
      "8              Hostel - 7.6km from city centre   3190 Total Reviews   \n",
      "9                Hostel - 7km from city centre    667 Total Reviews   \n",
      "10             Hostel - 5.5km from city centre     25 Total Reviews   \n",
      "11             Hostel - 5.5km from city centre   1790 Total Reviews   \n",
      "12             Hotel - 14.5km from city centre     19 Total Reviews   \n",
      "13  Bed and Breakfast - 4.2km from city centre     11 Total Reviews   \n",
      "14              Hotel - 5.3km from city centre   1381 Total Reviews   \n",
      "15              Hotel - 3.2km from city centre    115 Total Reviews   \n",
      "16              Hotel - 6.5km from city centre    278 Total Reviews   \n",
      "17             Hotel - 14.8km from city centre     58 Total Reviews   \n",
      "18             Hotel - 19.9km from city centre      0 Total Reviews   \n",
      "19              Hotel - 4.9km from city centre      0 Total Reviews   \n",
      "20              Hotel - 5.4km from city centre      2 Total Reviews   \n",
      "\n",
      "            Private Price          Dorm Price  \\\n",
      "1   No Privates Available    Rs6183.94 Rs5566   \n",
      "2   No Privates Available              Rs5509   \n",
      "3   No Privates Available              Rs5208   \n",
      "4                 Rs19631  No Dorms Available   \n",
      "5   No Privates Available              Rs5665   \n",
      "6   No Privates Available              Rs3767   \n",
      "7   No Privates Available              Rs4147   \n",
      "8   No Privates Available    Rs5427.94 Rs4885   \n",
      "9   No Privates Available     Rs5788.1 Rs5209   \n",
      "10     Rs31148.65 Rs26476    Rs9344.59 Rs7943   \n",
      "11     Rs17827.82 Rs16936  No Dorms Available   \n",
      "12                Rs18170  No Dorms Available   \n",
      "13                Rs22292  No Dorms Available   \n",
      "14     Rs26340.07 Rs18438  No Dorms Available   \n",
      "15                Rs18170  No Dorms Available   \n",
      "16                Rs14276  No Dorms Available   \n",
      "17                 Rs8858  No Dorms Available   \n",
      "18                 Rs8478  No Dorms Available   \n",
      "19                Rs23362  No Dorms Available   \n",
      "20                Rs26623  No Dorms Available   \n",
      "\n",
      "                                           Facilities  \\\n",
      "1    Free WiFi - Follows Covid-19 sanitation guidance   \n",
      "2    Free WiFi - Follows Covid-19 sanitation guidance   \n",
      "3    Free WiFi - Follows Covid-19 sanitation guidance   \n",
      "4    Free WiFi - Follows Covid-19 sanitation guidance   \n",
      "5    Free WiFi - Follows Covid-19 sanitation guidance   \n",
      "6   Free WiFi - Free Breakfast - Follows Covid-19 ...   \n",
      "7                                           Free WiFi   \n",
      "8    Free WiFi - Follows Covid-19 sanitation guidance   \n",
      "9    Free WiFi - Follows Covid-19 sanitation guidance   \n",
      "10                                          Free WiFi   \n",
      "11  Free WiFi - Free Breakfast - Follows Covid-19 ...   \n",
      "12  Free Breakfast - Follows Covid-19 sanitation g...   \n",
      "13                         Free WiFi - Free Breakfast   \n",
      "14                                          Free WiFi   \n",
      "15                                                      \n",
      "16                         Free WiFi - Free Breakfast   \n",
      "17  Free Breakfast - Follows Covid-19 sanitation g...   \n",
      "18                                          Free WiFi   \n",
      "19                                          Free WiFi   \n",
      "20                                          Free WiFi   \n",
      "\n",
      "                                          Description  \n",
      "1   Perfect Location - Superb Staff - Excellent Cl...  \n",
      "2   Perfect Location - Superb Staff - Fantastic Cl...  \n",
      "3   Perfect Location - Marvellous Staff - Awesome ...  \n",
      "4   Perfect Location - Superb Staff - Fantastic Cl...  \n",
      "5   Wonderful Location - Brilliant Staff - Awesome...  \n",
      "6   Wonderful Location - Superb Staff - Excellent ...  \n",
      "7               Wonderful Location - Marvellous Staff  \n",
      "8   Perfect Location - Marvellous Staff - Awesome ...  \n",
      "9                 Perfect Location - Marvellous Staff  \n",
      "10  Perfect Location - Superb Staff - Fantastic Cl...  \n",
      "11                Perfect Location - Marvellous Staff  \n",
      "12  Great Location - Superb Staff - Excellent Clea...  \n",
      "13  Perfect Location - Superb Staff - Fantastic Cl...  \n",
      "14  Perfect Location - Superb Staff - Fantastic Cl...  \n",
      "15  Perfect Location - Brilliant Staff - Awesome C...  \n",
      "16  Perfect Location - Superb Staff - Fantastic Cl...  \n",
      "17                                                     \n",
      "18                                                     \n",
      "19                                                     \n",
      "20                                                     \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "#driver.maximize_window()\n",
    "\n",
    "# Open hostelworld.com website on automated chrome window\n",
    "driver.get(\"https://www.hostelworld.com/\")\n",
    "\n",
    "time.sleep(2)\n",
    "driver.find_element_by_id(\"location-text-input-field\").click()\n",
    "driver.find_element_by_id(\"location-text-input-field\").send_keys(\"London\")\n",
    "time.sleep(2)\n",
    "driver.find_element_by_id(\"search-input-field\").send_keys(\"London\")\n",
    "time.sleep(5)\n",
    "driver.find_element_by_xpath(\"//div[@class='label']\").click()\n",
    "driver.find_element_by_id(\"search-button\").click()\n",
    "time.sleep(5)\n",
    "\n",
    "list_hostels = []\n",
    "list_HostelName = []\n",
    "list_Distance = []\n",
    "list_Rating = []\n",
    "list_TotalReviews = []\n",
    "list_OverallReviews = []\n",
    "list_PrivatePrice = []\n",
    "list_DormPrice = []\n",
    "list_Facilities = []\n",
    "list_Description = []\n",
    "\n",
    "# Get count of Featured Properties\n",
    "list_FeaturedHostels = driver.find_elements_by_xpath(\"//div[@class='featured-list']//div[@class='property-card']\")\n",
    "count_Featured = len(list_FeaturedHostels)\n",
    "\n",
    "list_hostels = driver.find_elements_by_xpath(\"//div[@class='property-card']\")\n",
    "\n",
    "# Remove Featured Properties from top of the property list\n",
    "for hostel in list_hostels[count_Featured:]:\n",
    "    \n",
    "    try:\n",
    "        list_HostelName.append(hostel.find_element_by_xpath(\".//h2[@class='title title-6']//a\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_HostelName.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        list_Distance.append(hostel.find_element_by_xpath(\".//span[@class='description']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_Distance.append(\"-\")   \n",
    " \n",
    "#     try:\n",
    "#         list_Rating.append(hostel.find_element_by_xpath(\".//div[contains(@class, 'score') and contains(@class, 'big')]\").text)\n",
    "#         #list_Rating.append(hostel.findElement(By.xpath(\"//div[contains(@class, 'score') and contains(@class, 'big')]\")).text, \"Rating\")\n",
    "#         #list_Rating.append(hostel.find_element_by_xpath(\"//div[contains(@class, 'score big')]\").text, \"Rating\")\n",
    "#     except NoSuchElementException:\n",
    "#         list_Rating.append(\"No Rating\")   \n",
    " \n",
    "    try:\n",
    "        list_TotalReviews.append(hostel.find_element_by_xpath(\".//div[@class='reviews']\").text)\n",
    "    except NoSuchElementException:\n",
    "        list_TotalReviews.append(\"-\")   \n",
    " \n",
    "#     try:\n",
    "#         list_OverallReviews.append(hostel.find_element_by_xpath(\"//h2[@class='title title-6']//a\").text)\n",
    "#     except NoSuchElementException:\n",
    "#         list_OverallReviews.append(\"-\")   \n",
    " \n",
    "    try:\n",
    "        price_Private = hostel.find_element_by_xpath(\".//div[@class='prices-col']//a//div[@class='price-col'][1]//p\").text\n",
    "        \n",
    "        if(price_Private == \"No Privates Available\"):\n",
    "            list_PrivatePrice.append(price_Private)\n",
    "        elif(price_Private == \"Privates From\"):\n",
    "            list_PrivatePrice.append(hostel.find_element_by_xpath(\".//div[@class='prices-col']//a//div[@class='price-col'][1]//div\").text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        list_PrivatePrice.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        price_Dorm = hostel.find_element_by_xpath(\".//div[@class='prices-col']//a//div[@class='price-col'][2]//p\").text\n",
    "        \n",
    "        if(price_Dorm == \"No Dorms Available\"):\n",
    "            list_DormPrice.append(price_Dorm)\n",
    "        elif(price_Dorm == \"Dorms From\"):\n",
    "            list_DormPrice.append(hostel.find_element_by_xpath(\".//div[@class='prices-col']//a//div[@class='price-col'][2]//div\").text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        list_DormPrice.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        list_Facilities.append(hostel.find_element_by_xpath(\".//div[@class='facilities-label facilities']\").text.replace(\"\\n\", \" - \"))\n",
    "    except NoSuchElementException:\n",
    "        list_Facilities.append(\"-\")   \n",
    " \n",
    "    try:\n",
    "        list_Description.append(hostel.find_element_by_xpath(\".//div[@class='rating-factors prop-card-tablet rating-factors small']\").text.replace(\"\\n\", \" - \"))\n",
    "    except NoSuchElementException:\n",
    "        list_Description.append(\"-\")\n",
    "  \n",
    "# Display all data in 1 table\n",
    "hostel_list = pd.DataFrame({'Hostel Name': list_HostelName, 'Distance': list_Distance, 'Total Reviews': list_TotalReviews, 'Private Price': list_PrivatePrice, 'Dorm Price': list_DormPrice, 'Facilities': list_Facilities, 'Description': list_Description})\n",
    "hostel_list.index += 1                                # Start Index from 1\n",
    "print(hostel_list)\n",
    "\n",
    "# Close automated chrome window\n",
    "driver.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
