{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504b3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e827373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries\n",
    "import selenium                   # Library that is use to work with selenium\n",
    "import pandas as pd               # To Create DataFrame\n",
    "from selenium import webdriver    # Importing webdriver module from selenium to open up automated chrome window\n",
    "import warnings                   # To Ignore any sort of warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time                       # Use to stop search engine for few seconds\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException     # Importing exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299ba86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d011b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb797a4e",
   "metadata": {},
   "source": [
    "### Q 1. Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "\n",
    "**Url** = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "       \n",
    "    You need to find following details: \n",
    "    A) Rank\n",
    "    B) Name\n",
    "    C) Artist\n",
    "    D) Upload date \n",
    "    E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2139ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open wikipedia.org website on automated chrome window for Details of Most Viewed Videos on YouTube\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "\n",
    "list_Ranks = []\n",
    "list_Names = []\n",
    "list_Artists = []\n",
    "list_UploadDates = []\n",
    "list_Views = []\n",
    "\n",
    "all_tables = driver.find_element_by_xpath(\"//table\")\n",
    "\n",
    "try:\n",
    "    # Most Viewed Videos\n",
    "    videos_table = all_tables.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']\")\n",
    "    \n",
    "    for video_table in videos_table[0:1]:\n",
    "        video_details = video_table.find_elements_by_xpath(\".//tbody//tr\")\n",
    "    \n",
    "    for video in video_details:\n",
    "    \n",
    "        try:\n",
    "            list_Ranks.append(video.find_element_by_xpath(\".//td[1]\").text.replace(\".\", \"\"))\n",
    "        except NoSuchElementException:\n",
    "            list_Ranks.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Names.append(video.find_element_by_xpath(\".//td[2]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Names.append(\"-\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            list_Artists.append(video.find_element_by_xpath(\".//td[3]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Artists.append(\"-\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            list_UploadDates.append(video.find_element_by_xpath(\".//td[5]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_UploadDates.append(\"-\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            list_Views.append(video.find_element_by_xpath(\".//td[4]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Views.append(\"-\")\n",
    "\n",
    "#     print(\"list_Ranks: \", len(list_Ranks), \" -- list_Names: \", len(list_Names), \" -- list_Artists: \", len(list_Artists), \" -- list_UploadDates: \", len(list_UploadDates), \" -- list_Views: \", len(list_Views)) \n",
    "\n",
    "\n",
    "    # Display all data in 1 table\n",
    "    videos_list = pd.DataFrame({'Rank': list_Ranks, 'Name': list_Names, 'Artist': list_Artists, 'Upload Date': list_UploadDates, 'Views (billions)': list_Views})\n",
    "    videos_list.index += 1                                # Start Index from 1\n",
    "    print(videos_list)\n",
    "\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n",
    "\n",
    "    \n",
    "except NoSuchElementException:\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63116516",
   "metadata": {},
   "source": [
    "### Q 2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "\n",
    "**Url** = https://www.bcci.tv/.\n",
    "     \n",
    "     You need to find following details:\n",
    "     A) Match title (I.e. 1st ODI)\n",
    "     B) Series\n",
    "     C) Place\n",
    "     D) Date\n",
    "     E) Time\n",
    "**Note:** - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab663e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open bcci.tv website on automated chrome window\n",
    "driver.get(\"https://www.bcci.tv/\")\n",
    "\n",
    "#time.sleep(5)\n",
    "\n",
    "# Go to Team India’s International Fixtures page\n",
    "driver.find_element_by_xpath(\"//ul[@class='nav navbar-nav  align-items-center']//li[@class='nav-item']//a[text()='INTERNATIONAL']\").click()\n",
    "\n",
    "list_MatchTitles = []\n",
    "list_Series = []\n",
    "list_Places = []\n",
    "list_Dates = []\n",
    "list_Times = []\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "try:\n",
    "    \n",
    "    matches_table = driver.find_element_by_xpath(\"//div[@class='fixture-tab-inner row']\")\n",
    "    matches_details = matches_table.find_elements_by_xpath(\".//div[@class='fixture-card-main col-lg-3 col-md-6 col-sm-12 ng-scope']\")\n",
    "\n",
    "    for match in matches_details:\n",
    "\n",
    "        matchTitle = \"\"\n",
    "        \n",
    "        try:\n",
    "            matchTitle = match.find_element_by_xpath(\".//span[@class='matchOrderText ng-binding ng-scope']\").text\n",
    "            \n",
    "            list_MatchTitles.append(matchTitle.replace(\" -\", \"\"))\n",
    "        except NoSuchElementException:\n",
    "            list_MatchTitles.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Series.append(match.find_element_by_xpath(\".//div[@class='fixture-card-top']//h5//span[@class='ng-binding']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Series.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Places.append(match.find_element_by_xpath(\".//div[@class='fix-place ng-binding ng-scope']\").text.replace(matchTitle, \"\"))\n",
    "        except NoSuchElementException:\n",
    "            list_Places.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Dates.append(match.find_element_by_xpath(\".//div[@class='match-card-left match-schedule']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Dates.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Times.append(match.find_element_by_xpath(\".//div[@class='match-card-right match-schedule ']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Times.append(\"-\")\n",
    "\n",
    "\n",
    "    # Display all data in 1 table\n",
    "    matches_list = pd.DataFrame({'Match Title': list_MatchTitles, 'Series': list_Series, 'Place': list_Places, 'Date': list_Dates, 'Time': list_Times})\n",
    "    matches_list.index += 1                                # Start Index from 1\n",
    "    print(matches_list)\n",
    "\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n",
    "\n",
    "except NoSuchElementException:\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9482bb",
   "metadata": {},
   "source": [
    "### Q 3. Scrape the details of selenium exception from guru99.com.\n",
    "\n",
    "**Url** = https://www.guru99.com/\n",
    "\n",
    "    You need to find following details:\n",
    "    A) Name\n",
    "    B) Description\n",
    "**Note:** - From guru99 home page you have to reach to selenium exception handling page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b0fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open guru99.com website on automated chrome window\n",
    "driver.get(\"https://www.guru99.com/\")\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Go to Selenium Page\n",
    "driver.find_element_by_xpath(\"//ul[@class='menu1']//li//a[@title='Selenium']\").click()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Go to Selenium Exception Handling\n",
    "driver.find_element_by_xpath(\"//table[@class='table']//tbody//tr//td[@class='responsivetable']//a[@title='Selenium Exception Handling (Common Exceptions List)']\").click()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "try:\n",
    "    exceptions_table = driver.find_element_by_xpath(\"//table[@class='table table-striped']\")\n",
    "    exceptions_details = exceptions_table.find_elements_by_xpath(\".//tbody//tr\")\n",
    "    \n",
    "    list_Names = []\n",
    "    list_Description = []\n",
    "    \n",
    "    for exception in exceptions_details:\n",
    "        \n",
    "        try:\n",
    "            list_Names.append(exception.find_element_by_xpath(\".//td[1]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Names.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Description.append(exception.find_element_by_xpath(\".//td[2]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Description.append(\"-\")\n",
    "\n",
    "\n",
    "    # Display all data in 1 table\n",
    "    exceptions_list = pd.DataFrame({'Name': list_Names[1:], 'Description': list_Description[1:]})\n",
    "    exceptions_list.index += 1                                # Start Index from 1\n",
    "    print(exceptions_list)\n",
    "\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n",
    "\n",
    "except NoSuchElementException:\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2924e",
   "metadata": {},
   "source": [
    "### Q 4. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "**Url** = http://statisticstimes.com/\n",
    "\n",
    "    You have to find following details:\n",
    "    A) Rank\n",
    "    B) State\n",
    "    C) GSDP(18-19)\n",
    "    D) GSDP(17-18)\n",
    "    E) Share(2017)\n",
    "    F) GDP($ billion)\n",
    "**Note:** - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637bf612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open statisticstimes.com website on automated chrome window\n",
    "driver.get(\"http://statisticstimes.com/\")\n",
    "\n",
    "try:\n",
    "    \n",
    "    #object of ActionChains\n",
    "    a = ActionChains(driver)\n",
    "\n",
    "    #identify element (Select meu-item 'Economy')\n",
    "    m = driver.find_element_by_xpath(\"//div[@class='navbar']//div[@class='dropdown'][2]//button[@class='dropbtn']\")\n",
    "    #hover over element\n",
    "    a.move_to_element(m).perform()\n",
    "    time.sleep(1)\n",
    "\n",
    "    #identify sub menu element (Select submenu-item 'India')\n",
    "    n = driver.find_element_by_link_text(\"India\")\n",
    "    # hover over element and click\n",
    "    a.move_to_element(n).click().perform()\n",
    "    time.sleep(1)\n",
    "\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//span[text()='Close']\").click()\n",
    "    except NoSuchElementException:\n",
    "        time.sleep(5)\n",
    "\n",
    "    # Go to 'GDP of Indian States' for State-wise GDP of India\n",
    "    driver.find_element_by_xpath(\"//*[contains(text(), 'GDP of Indian states')]\").click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    gdp_table = driver.find_element_by_xpath(\"//table[@id='table_id']\")\n",
    "    gdp_details = gdp_table.find_elements_by_xpath(\".//tbody//tr\")\n",
    "\n",
    "    list_Rank = []\n",
    "    list_State = []\n",
    "    list_GSDP18_19 = []\n",
    "    list_GSDP19_20 = []\n",
    "    list_Share = []\n",
    "    list_GDP = []\n",
    "\n",
    "    for gdp_state in gdp_details:\n",
    "\n",
    "        try:\n",
    "            list_Rank.append(gdp_state.find_element_by_xpath(\".//td[1]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Rank.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_State.append(gdp_state.find_element_by_xpath(\".//td[2]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_State.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_GSDP19_20.append(gdp_state.find_element_by_xpath(\".//td[3]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_GSDP19_20.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_GSDP18_19.append(gdp_state.find_element_by_xpath(\".//td[4]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_GSDP18_19.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Share.append(gdp_state.find_element_by_xpath(\".//td[5]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Share.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_GDP.append(gdp_state.find_element_by_xpath(\".//td[6]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_GDP.append(\"-\")\n",
    "\n",
    "\n",
    "    # Display all data in 1 table\n",
    "    gdp_list = pd.DataFrame({'Rank': list_Rank, 'State': list_State, 'GSDP (19-20)': list_GSDP19_20, 'GSDP (18-19)': list_GSDP18_19, 'Share': list_Share, 'GDP ($billion)': list_GDP})\n",
    "    gdp_list.index += 1                                # Start Index from 1\n",
    "    print(gdp_list)\n",
    "\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n",
    "\n",
    "except NoSuchElementException:\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df920951",
   "metadata": {},
   "source": [
    "### Q 5. Scrape the details of trending repositories on Github.com.\n",
    "**Url** = https://github.com/\n",
    "\n",
    "    You have to find the following details:\n",
    "    A) Repository title\n",
    "    B) Repository description \n",
    "    C) Contributors count\n",
    "    D) Language used\n",
    "**Note:** - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00912ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open github.com website on automated chrome window\n",
    "driver.get(\"https://github.com/\")\n",
    "\n",
    "try:\n",
    "    #object of ActionChains\n",
    "    a = ActionChains(driver)\n",
    "    \n",
    "    #identify element (Select menu-item 'Explore')\n",
    "    m = driver.find_element_by_xpath(\"//*[contains(text(), 'Explore')]\")\n",
    "    #hover over element\n",
    "    a.move_to_element(m).perform()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #identify sub menu element (Select submenu-item 'Trending')\n",
    "    n = driver.find_element_by_link_text(\"Trending\")\n",
    "    # hover over element and click\n",
    "    a.move_to_element(n).click().perform()\n",
    "    time.sleep(1)\n",
    "\n",
    "    repository_details = driver.find_elements_by_xpath(\"//article[@class='Box-row']\")\n",
    "    print(\"repository_details: \", len(repository_details))\n",
    "\n",
    "    list_RepositoryTitle = []\n",
    "    list_RepositoryDescription = []\n",
    "    list_ContributorsCount = []\n",
    "    list_Language = []\n",
    "\n",
    "    for repository in repository_details:\n",
    "\n",
    "        try:\n",
    "            list_RepositoryTitle.append(repository.find_element_by_xpath(\".//h1[@class='h3 lh-condensed']//a\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_RepositoryTitle.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_RepositoryDescription.append(repository.find_element_by_xpath(\".//p\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_RepositoryDescription.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_ContributorCount = repository.find_elements_by_xpath(\".//span[@class='d-inline-block mr-3']//a\")\n",
    "            list_ContributorsCount.append(len(list_ContributorCount))\n",
    "        except NoSuchElementException:\n",
    "            list_ContributorsCount.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Language.append(repository.find_element_by_xpath(\".//span[@itemprop='programmingLanguage']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Language.append(\"-\")\n",
    "\n",
    "    # Display all data in 1 table\n",
    "    repository_list = pd.DataFrame({'Repository Title': list_RepositoryTitle, 'Repository Description': list_RepositoryDescription, 'Contributors Count': list_ContributorsCount, 'Language Used': list_Language})\n",
    "    repository_list.index += 1                                # Start Index from 1\n",
    "    print(repository_list)\n",
    "\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n",
    "\n",
    "except NoSuchElementException:\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca65288",
   "metadata": {},
   "source": [
    "### Q 6. Scrape the details of top 100 songs on billiboard.com.\n",
    "**Url** = https:/www.billboard.com/\n",
    "\n",
    "    You have to find the following details:\n",
    "    A) Song name\n",
    "    B) Artist name\n",
    "    C) Last week rank\n",
    "    D) Peak rank\n",
    "    E) Weeks on board\n",
    "**Note:** - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open billboard.com website on automated chrome window\n",
    "driver.get(\"https:/www.billboard.com/\")\n",
    "\n",
    "# Go to 'Charts'\n",
    "driver.find_element_by_xpath(\"//nav[@class='o-nav  ']//ul//li//a[contains(text(), 'Charts')]\").click()\n",
    "\n",
    "# Go to 'Hot 100' for Top 100 Songs List\n",
    "driver.find_element_by_xpath(\"//nav[@class='o-nav  ']//ul//li//a[contains(text(), 'Hot 100')]\").click()\n",
    "\n",
    "try:\n",
    "    topSongs_details = driver.find_elements_by_xpath(\"//div[@class='o-chart-results-list-row-container']\")\n",
    "    print(\"topSongs_details: \", len(topSongs_details))\n",
    "    \n",
    "    list_Song = []\n",
    "    list_Artist = []\n",
    "    list_LastWeekRank = []\n",
    "    list_PeakRank = []\n",
    "    list_WeeksOnBoard = []\n",
    "    \n",
    "    for topSong in topSongs_details:\n",
    "\n",
    "        try:\n",
    "            list_Song.append(topSong.find_element_by_xpath(\".//li[@class='lrv-u-width-100p']//ul//li[1]//h3[@id='title-of-a-story']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Song.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Artist.append(topSong.find_element_by_xpath(\".//li[@class='lrv-u-width-100p']//ul//li[1]//span\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Artist.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_LastWeekRank.append(topSong.find_element_by_xpath(\".//li[@class='lrv-u-width-100p']//ul//li[4]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_LastWeekRank.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_PeakRank.append(topSong.find_element_by_xpath(\".//li[@class='lrv-u-width-100p']//ul//li[5]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_PeakRank.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_WeeksOnBoard.append(topSong.find_element_by_xpath(\".//li[@class='lrv-u-width-100p']//ul//li[6]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_WeeksOnBoard.append(\"-\")\n",
    "\n",
    "    # Display all data in 1 table\n",
    "    songs_list = pd.DataFrame({'Song Name': list_Song, 'Artist Name': list_Artist, 'Last Week Rank': list_LastWeekRank, 'Peak Rank': list_PeakRank, 'Weeks On Board': list_WeeksOnBoard})\n",
    "    songs_list.index += 1                                # Start Index from 1\n",
    "    print(songs_list)\n",
    "\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n",
    "\n",
    "except NoSuchElementException:\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e1e08",
   "metadata": {},
   "source": [
    "### Q 7. Scrape the details of Data science recruiters from naukri.com. \n",
    "**Url** = https://www.naukri.com/\n",
    "\n",
    "    You have to find the following details:\n",
    "    A) Name\n",
    "    B) Designation\n",
    "    C) Company\n",
    "    D) Skills they hire for\n",
    "    E) Location\n",
    "**Note:** - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open naukri.com website on automated chrome window for Details of Data Science Recruiters\n",
    "driver.get(\"https://www.naukri.com/nlogin/login\")\n",
    "\n",
    "driver.find_element_by_xpath(\"//a[@title='Search Recruiters']\").click()\n",
    "time.sleep(5)\n",
    "\n",
    "try:\n",
    "    search_field = driver.find_element_by_xpath(\"//input[@name='qp']\")\n",
    "    search_field.send_keys(\"Data science\")\n",
    "    driver.find_element_by_xpath(\"//button[text()='Search ']\")\n",
    "    time.sleep(2)\n",
    "except NoSuchElementException:\n",
    "    # Switch Focus on New Tab\n",
    "    \n",
    "    #get current window handle\n",
    "    p = driver.current_window_handle\n",
    "    #get first child window\n",
    "    chwd = driver.window_handles\n",
    "    \n",
    "    for w in chwd:\n",
    "        if(w!=p):\n",
    "            driver.switch_to.window(w)\n",
    "            break\n",
    "            time.sleep(1)\n",
    "            \n",
    "search_field = driver.find_element_by_xpath(\"//input[@name='qp']\")\n",
    "search_field.send_keys(\"Data science\")\n",
    "driver.find_element_by_xpath(\"//button[text()='Search ']\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "try:\n",
    "    recruiters_table = driver.find_element_by_xpath(\"//div[@id='tabP-1']\")\n",
    "    recruiters_details = recruiters_table.find_elements_by_xpath(\".//div[@class='recInfo']\")\n",
    "    \n",
    "    list_Name = []\n",
    "    list_Designation = []\n",
    "    list_Company = []\n",
    "    list_SkillsTheyHireFor = []\n",
    "    list_Location = []\n",
    "    \n",
    "    for recruiter in recruiters_details:\n",
    "\n",
    "        try:\n",
    "            list_Name.append(recruiter.find_element_by_xpath(\".//div[@class='vcard']/p[@class='highlightable']/a[@class='ellipsis'][1]/span[@class='fl ellipsis']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Name.append(\"-\")\n",
    "            \n",
    "        try:\n",
    "            list_Designation.append(recruiter.find_element_by_xpath(\".//div[@class='vcard']/p[@class='highlightable']/span[@class='ellipsis clr']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Designation.append(\"-\")\n",
    "            \n",
    "        try:\n",
    "            list_Company.append(recruiter.find_element_by_xpath(\".//div[@class='vcard']/p[@class='highlightable']/a[@class='ellipsis'][2]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Company.append(\"-\")\n",
    "            \n",
    "        try:\n",
    "            list_SkillsTheyHireFor.append(recruiter.find_element_by_xpath(\".//div[@class='hireSec highlightable']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_SkillsTheyHireFor.append(\"-\")\n",
    "            \n",
    "        try:\n",
    "            list_Location.append(recruiter.find_element_by_xpath(\".//div[@class='vcard']/p[@class='highlightable']/span[2]/small[@class='ellipsis']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Location.append(\"-\")\n",
    "\n",
    "    # Display all data in 1 table\n",
    "    recruiters_list = pd.DataFrame({'Name': list_Name, 'list_Designation': list_Designation, 'Company': list_Company, 'Skills They Hire For': list_SkillsTheyHireFor, 'Location': list_Location})\n",
    "    recruiters_list.index += 1                                # Start Index from 1\n",
    "    print(recruiters_list)\n",
    "            \n",
    "    # Close automated chrome window\n",
    "    driver.quit()\n",
    "\n",
    "except NoSuchElementException:\n",
    "    # Close automated chrome window\n",
    "    driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb86b9",
   "metadata": {},
   "source": [
    "### Q 8. Scrape the details of Highest selling novels.\n",
    "**Url** = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\n",
    "\n",
    "    You have to find the following details:\n",
    "    A) Book name\n",
    "    B) Author name\n",
    "    C) Volumes sold\n",
    "    D) Publisher\n",
    "    E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c00e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open theguardian.com website on automated chrome window for Details of Highest Selling Novels\n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\")\n",
    "\n",
    "try:\n",
    "    novels_table = driver.find_element_by_xpath(\"//table[@class='in-article sortable']\")\n",
    "    novels_details = novels_table.find_elements_by_xpath(\".//tbody//tr\")\n",
    "    \n",
    "    list_Book = []\n",
    "    list_Author = []\n",
    "    list_VolumesSold = []\n",
    "    list_Publisher = []\n",
    "    list_Genre = []\n",
    "    \n",
    "    for novel in novels_details:\n",
    "\n",
    "        try:\n",
    "            list_Book.append(novel.find_element_by_xpath(\".//td[2]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Book.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Author.append(novel.find_element_by_xpath(\".//td[3]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Author.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_VolumesSold.append(novel.find_element_by_xpath(\".//td[4]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_VolumesSold.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Publisher.append(novel.find_element_by_xpath(\".//td[5]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Publisher.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Genre.append(novel.find_element_by_xpath(\".//td[6]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Genre.append(\"-\")\n",
    "\n",
    "    \n",
    "    # Display all data in 1 table\n",
    "    novels_list = pd.DataFrame({'Book Name': list_Book, 'Author Name': list_Author, 'Volumes Sold': list_VolumesSold, 'Publisher': list_Publisher, 'Genre': list_Genre})\n",
    "    novels_list.index += 1                                # Start Index from 1\n",
    "    print(novels_list)\n",
    "            \n",
    "    # Close automated chrome window\n",
    "    driver.close()\n",
    "\n",
    "except NoSuchElementException:\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952daefe",
   "metadata": {},
   "source": [
    "### Q 9. Scrape the details most watched tv series of all time from imdb.com. \n",
    "**Url** = https://www.imdb.com/list/ls095964455/\n",
    "\n",
    "    You have to find the following details: \n",
    "    A) Name\n",
    "    B) Year span\n",
    "    C) Genre\n",
    "    D) Run time \n",
    "    E) Ratings \n",
    "    F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386dd379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open imdb.com website on automated chrome window for Details of Most Watched TV Series of All Time\n",
    "driver.get(\"https://www.imdb.com/list/ls095964455/\")\n",
    "\n",
    "try:\n",
    "    tvShow_details = driver.find_elements_by_xpath(\"//div[@class='lister-item mode-detail']\")\n",
    "    \n",
    "    list_Name = []\n",
    "    list_YearSpan = []\n",
    "    list_Genre = []\n",
    "    list_RunTime = []\n",
    "    list_Rating = []\n",
    "    list_Votes = []\n",
    "    \n",
    "    for tvShow in tvShow_details:\n",
    "\n",
    "        try:\n",
    "            list_Name.append(tvShow.find_element_by_xpath(\".//h3[@class='lister-item-header']//a\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Name.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_YearSpan.append(tvShow.find_element_by_xpath(\".//span[@class='lister-item-year text-muted unbold']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_YearSpan.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Genre.append(tvShow.find_element_by_xpath(\".//p[@class='text-muted text-small']//span[@class='genre']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Genre.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_RunTime.append(tvShow.find_element_by_xpath(\".//p[@class='text-muted text-small']//span[@class='runtime']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_RunTime.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Rating.append(tvShow.find_element_by_xpath(\".//span[@class='ipl-rating-star__rating']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Rating.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Votes.append(tvShow.find_element_by_xpath(\".//span[@name='nv']\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Votes.append(\"-\")\n",
    "\n",
    "    \n",
    "    # Display all data in 1 table\n",
    "    tvShow_list = pd.DataFrame({'Name': list_Name, 'Year Span': list_YearSpan, 'Genre': list_Genre, 'Run Time': list_RunTime, 'Rating': list_Rating, 'Votes': list_Votes})\n",
    "    tvShow_list.index += 1                                # Start Index from 1\n",
    "    print(tvShow_list)\n",
    "            \n",
    "    # Close automated chrome window\n",
    "    driver.close()\n",
    "\n",
    "except NoSuchElementException:\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6186846",
   "metadata": {},
   "source": [
    "### Q 10. Details of Datasets from UCI machine learning repositories. \n",
    "**Url** = https://archive.ics.uci.edu/\n",
    "\n",
    "    You have to find the following details:\n",
    "    A) Dataset name \n",
    "    B) Data type\n",
    "    C) Task\n",
    "    D) Attribute type \n",
    "    E) No of instances \n",
    "    F) No of attribute \n",
    "    G) Year\n",
    "**Note:** - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dacd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open archive.ics.uci.edu website on automated chrome window for Details of Datasets\n",
    "driver.get(\"https://archive.ics.uci.edu/\")\n",
    "\n",
    "time.sleep(2)\n",
    "# Go to View All Datasets\n",
    "driver.find_element_by_xpath(\"/html/body/table[1]/tbody/tr/td[2]/span[2]/a\").click()\n",
    "\n",
    "try:\n",
    "    dataset_table = driver.find_element_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]\")\n",
    "    dataset_details = dataset_table.find_elements_by_xpath(\".//tbody//tr\")\n",
    "    print(\"dataset_details: \", len(dataset_details))\n",
    "    \n",
    "    list_DataSet = []\n",
    "    list_DataType = []\n",
    "    list_Task = []\n",
    "    list_AttributeType = []\n",
    "    list_InstancesNo = []\n",
    "    list_AttributeNo = []\n",
    "    list_Year = []\n",
    "    \n",
    "    for dataset in dataset_details[1:100]:\n",
    "        \n",
    "        try:\n",
    "            list_DataSet.append(dataset.find_element_by_xpath(\".//td[1]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_DataSet.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_DataType.append(dataset.find_element_by_xpath(\".//td[2]/p\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_DataType.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Task.append(dataset.find_element_by_xpath(\".//td[3]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Task.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_AttributeType.append(dataset.find_element_by_xpath(\".//td[4]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_AttributeType.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_InstancesNo.append(dataset.find_element_by_xpath(\".//td[5]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_InstancesNo.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_AttributeNo.append(dataset.find_element_by_xpath(\".//td[6]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_AttributeNo.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            list_Year.append(dataset.find_element_by_xpath(\".//td[7]\").text)\n",
    "        except NoSuchElementException:\n",
    "            list_Year.append(\"-\")\n",
    "    print(\"Start\")\n",
    "    \n",
    "    # Display all data in 1 table\n",
    "    dataset_list = pd.DataFrame({'DataSet Name': list_DataSet, 'Data Type': list_DataType, 'Task': list_Task, 'Attribute Type': list_AttributeType, 'No of Instances': list_InstancesNo, 'No of Attribute': list_AttributeNo, 'Year': list_Year})\n",
    "    dataset_list.index += 1                                # Start Index from 1\n",
    "    print(dataset_list)\n",
    "            \n",
    "    print(\"End\")\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n",
    "\n",
    "except NoSuchElementException:\n",
    "    # Close automated chrome window\n",
    "    driver.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267373cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e76428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
