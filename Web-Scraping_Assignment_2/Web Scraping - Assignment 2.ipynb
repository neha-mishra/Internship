{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f3f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcbaa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries\n",
    "import selenium                   # Library that is use to work with selenium\n",
    "import pandas as pd               # To Create DataFrame\n",
    "from selenium import webdriver    # Importing webdriver module from selenium to open up automated chrome window\n",
    "import warnings                   # To Ignore any sort of warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time                       # Use to stop search engine for few seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ebef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2910495",
   "metadata": {},
   "source": [
    "### Q 1. Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccbf28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open naukri.com website on automated chrome window\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "# Finding element for Job Search Bar and Entering position Data Analyst\n",
    "search_field_designation = driver.find_element_by_class_name(\"suggestor-input \")      \n",
    "search_field_designation.send_keys(\"Data Analyst\")\n",
    "\n",
    "# Finding element for Location Search Bar and Entering loaction Bangalore\n",
    "search_field_location = driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input\")  # Location Search Bar\n",
    "search_field_location.send_keys(\"Bangalore\")\n",
    "\n",
    "# Clicking on Search Button\n",
    "search_button = driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "search_button.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Scrape Job Title\n",
    "job_titles = []\n",
    "title_tags = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")        # Locating web element of Job Title\n",
    "for title in title_tags:                                                                # Iterating over web elements of Job Title\n",
    "    job_titles.append(title.text)                                                       # Appending extracted text from each web element into empty list\n",
    "\n",
    "# Scrape Job Location\n",
    "locations_list = []\n",
    "location_tags = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "for location in location_tags:\n",
    "    locations_list.append(location.text)\n",
    "\n",
    "# Scrape Company Name\n",
    "company_names = []\n",
    "company_tags = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for company_name in company_tags:\n",
    "    company_names.append(company_name.text)\n",
    "\n",
    "# Scrape Experience Required\n",
    "experience_list = []\n",
    "experience_tags = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\")\n",
    "for experience in experience_tags:\n",
    "    experience_list.append(experience.text)\n",
    "\n",
    "# Display all data in 1 table\n",
    "jobs = pd.DataFrame({'Job Title': job_titles, 'Job Location': locations_list, 'Company Name': company_names, 'Experience Required': experience_list})\n",
    "jobs.index += 1                                # Start Index from 1\n",
    "\n",
    "# Keep only top 10 results\n",
    "jobs[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b1a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c6c51",
   "metadata": {},
   "source": [
    "### Q 2. Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a6d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open naukri.com website on automated chrome window\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "# Finding element for Job Search Bar and Entering position Data Scientist\n",
    "search_field_designation = driver.find_element_by_class_name(\"suggestor-input \")\n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "# Finding element for Location Search Bar and Entering loaction Bangalore\n",
    "search_field_location = driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input\")  # Location Search Bar\n",
    "search_field_location.send_keys(\"Bangalore\")\n",
    "\n",
    "# Clicking on Search Button\n",
    "search_button = driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "search_button.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Scrape Job Title\n",
    "job_titles = []\n",
    "title_tags = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")        # Locating web elements of Job Title\n",
    "for title in title_tags:                                                                # Iterating over web elements of Job Title\n",
    "    job_titles.append(title.text)                                                       # Appending each extracted text into empty list\n",
    "\n",
    "# Scrape Job Location\n",
    "locations_list = []\n",
    "location_tags = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")        # Locating web elements of Location\n",
    "for location in location_tags:                                                          # Iterating over web elements of Location\n",
    "    locations_list.append(location.text)                                                # Appending each extracted text into empty list\n",
    "\n",
    "# Scrape Company Name\n",
    "company_names = []\n",
    "company_tags = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")   # Locating web elements of Company Name\n",
    "for company_name in company_tags:                                                       # Iterating over web elements of Company Name\n",
    "    company_names.append(company_name.text)                                             # Appending each extracted text into empty list\n",
    "\n",
    "# Display all data in 1 table\n",
    "jobs = pd.DataFrame({'Job Title': job_titles, 'Job Location': locations_list, 'Company Name': company_names})\n",
    "jobs.index += 1                                # Start Index from 1\n",
    "\n",
    "# Keep only top 10 results\n",
    "jobs[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529dc61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463790e5",
   "metadata": {},
   "source": [
    "### Q 3. In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "###### You have to use the location and salary filter.\n",
    "###### You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "###### You have to scrape the job-title, job-location, company name, experience required. The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95f2bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open naukri.com website on automated chrome window\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "# Finding element for Job Search Bar and Entering position Data Scientist\n",
    "search_field_designation = driver.find_element_by_class_name(\"suggestor-input \")\n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "# Clicking on Search Button\n",
    "search_button = driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# Selecting Salary Filter as 3-6 Lakhs\n",
    "driver.find_element_by_xpath(\".//*[contains(text(), '3-6 Lakhs')]\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "# Selecting Location Filter as Delhi/NCR\n",
    "driver.find_element_by_xpath(\".//*[contains(text(), 'Delhi / NCR')]\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "# Scrape Job Title\n",
    "job_titles = []\n",
    "title_tags = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")        # Locating web element of Job Title\n",
    "for title in title_tags:                                                                # Iterating over web elements of Job Title\n",
    "    job_titles.append(title.text)                                                       # Appending each extracted text into empty list\n",
    "\n",
    "# Scrape Job Location\n",
    "locations_list = []\n",
    "location_tags = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "for location in location_tags:                                                          # Iterating over web elements of Location\n",
    "    locations_list.append(location.text)                                                # Appending each extracted text into empty list\n",
    "\n",
    "# Scrape Company Name\n",
    "company_names = []\n",
    "company_tags = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")   # Locating web elements of Company Name\n",
    "for company_name in company_tags:                                                       # Iterating over web elements of Company Name\n",
    "    company_names.append(company_name.text)                                             # Appending each extracted text into empty list\n",
    "\n",
    "# Scrape Experience Required\n",
    "experience_list = []\n",
    "experience_tags = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\")   # Locating web elements of Experience Required\n",
    "for experience in experience_tags:                                                      # Iterating over web elements of Experience Required\n",
    "    experience_list.append(experience.text)                                             # Appending each extracted text into empty list\n",
    "\n",
    "# Display all data in 1 table\n",
    "jobs = pd.DataFrame({'Job Title': job_titles, 'Job Location': locations_list, 'Company Name': company_names, 'Experience Required': experience_list})\n",
    "jobs.index += 1                                # Start Index from 1\n",
    "\n",
    "# Keep only top 10 results\n",
    "jobs[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3713cb",
   "metadata": {},
   "source": [
    "### Q 4. Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes: \n",
    "###### 1. Brand\n",
    "###### 2. Product Description\n",
    "###### 3. Price\n",
    "###### 4. Discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open flipkart.com website on automated chrome window\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "# Close Login Pop-up\n",
    "driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "# Finding element for Product Search Bar and Entering product Sunglasses\n",
    "search_field_product = driver.find_element_by_class_name(\"_3704LK\")\n",
    "search_field_product.send_keys(\"Sunglasses\")\n",
    "\n",
    "# Clicking on Search Button\n",
    "search_button = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "brand_list = []                   # Product Brand\n",
    "description_list = []             # Product Description\n",
    "price_list = []                   # Price\n",
    "discount_list = []                # Discount\n",
    "\n",
    "for i in range(0, 3):             # Running for loop with range to run this loop 2 times because each search returns 40 results\n",
    "\n",
    "    # Scrap Product Brand\n",
    "    brand_tags = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")                   # Locating web element of Brand\n",
    "    for brand in brand_tags:                                                                # Iterating over web elements of Brand\n",
    "        brand_list.append(brand.text)                                                       # Appending each extracted text into empty list\n",
    "\n",
    "    # Scrap Product Description\n",
    "    description_tags = driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[1]\")        # Locating web element of Description\n",
    "    for description in description_tags:                                                    # Iterating over web elements of Description\n",
    "        description_list.append(description.text)                                           # Appending each extracted text into empty list\n",
    "            \n",
    "    # Scrap Price\n",
    "    price_tags = driver.find_elements_by_xpath(\"//a[@class='_3bPFwb']/div[1]/div[1]\")       # Locating web elements of Price\n",
    "    for price in price_tags:                                                                # Iterating over web elements of Price\n",
    "        price_list.append(price.text)                                                       # Appending each extracted text into empty list\n",
    "\n",
    "    # Scrap Discount\n",
    "    discount_tags = driver.find_elements_by_xpath(\"//a[@class='_3bPFwb']/div/div[3]\")       # Locating web elements of Discount\n",
    "    for discount in discount_tags:                                                          # Iterating over web elements of Discount\n",
    "        discount_list.append(discount.text)                                                 # Appending each extracted text into empty list\n",
    "\n",
    "    # Clicking on Next Button\n",
    "    if(i < 2):                              # Skip clicking Next Button last time because we only need top 100 results\n",
    "        driver.find_element_by_xpath(\".//*[contains(text(), 'Next')]\").click()              # Locating web element of next button and then clicking it\n",
    "    time.sleep(5)                           # Using time to pause the search engine for 5 second\n",
    "      \n",
    "# Display all data in 1 table\n",
    "products = pd.DataFrame({'Brand': brand_list, 'Product Description': description_list, 'Price': price_list, 'Discount': discount_list})\n",
    "products.index += 1                         # Start Index from 1\n",
    "\n",
    "# Keep only top 100 results\n",
    "products[0:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b7be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b67ea",
   "metadata": {},
   "source": [
    "### Q 5. Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to scrape following attributes:\n",
    "###### 1. Rating\n",
    "###### 2. Review Summary\n",
    "###### 3. Full Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open flipkart.com website on automated chrome window\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power- adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC TSVZAXUHGREPBFGI&marketplace.\")\n",
    "\n",
    "# Clicking on All Reviews\n",
    "allReviews_button = driver.find_element_by_xpath(\"//div[@class='col JOpGWq']/a\").click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "rating_list = []                     # Rating\n",
    "review_summery_list = []             # Review Summary\n",
    "full_review_list = []                # Full Review\n",
    "\n",
    "\n",
    "for i in range(0, 10):          # Running for loop with range to run this loop 10 times because each search returns 10 results\n",
    "\n",
    "    # Scrap Review Rating\n",
    "    rating_tags = driver.find_elements_by_xpath(\"//div[@class='col _2wzgFH K0kLPL']/div[1]/div[1]\")        # Locating web element of Rating\n",
    "    for rating in rating_tags:                                                              # Iterating over web elements of Rating\n",
    "        rating_list.append(rating.text)                                                     # Appending each extracted text into empty list\n",
    "\n",
    "    # Scrap Review Summary\n",
    "    summery_tags = driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")                   # Locating web element of Summary\n",
    "    for summery in summery_tags:                                                            # Iterating over web elements of Summary\n",
    "        review_summery_list.append(summery.text)                                            # Appending each extracted text into empty list\n",
    "            \n",
    "    # Scrap Full Review\n",
    "    review_tags = driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div/div\")           # Locating web elements of Review\n",
    "    for review in review_tags:                                                              # Iterating over web elements of Review\n",
    "        full_review_list.append(review.text)                                                # Appending each extracted text into empty list\n",
    "\n",
    "    # Clicking on Next Button\n",
    "    if(i < 10):                              # Skip clicking Next Button last time because we only need top 100 results\n",
    "        driver.find_element_by_xpath(\".//*[contains(text(), 'Next')]\").click()              # Locating web element of next button and then clicking it\n",
    "    time.sleep(5)                            # Using time to pause the search engine for 5 second\n",
    "      \n",
    "# Display all data in 1 table\n",
    "products = pd.DataFrame({'Rating': rating_list, 'Review Summary': review_summery_list, 'Full Review': full_review_list})\n",
    "products.index += 1                          # Start Index from 1\n",
    "products\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f4e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243c92f",
   "metadata": {},
   "source": [
    "### Q 6. Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field. You have to scrape following attributes:\n",
    "###### 1. Brand\n",
    "###### 2. Product Description\n",
    "###### 3. Price\n",
    "###### 4. Discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open flipkart.com website on automated chrome window\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "# Close Login Pop-up\n",
    "driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "# Finding element for Product search bar\n",
    "search_field_product = driver.find_element_by_class_name(\"_3704LK\")\n",
    "search_field_product.send_keys(\"sneakers\")\n",
    "\n",
    "# Clicking on Search Button\n",
    "search_button = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "brand_list = []                   # Product Brand\n",
    "description_list = []             # Product Description\n",
    "price_list = []                   # Price\n",
    "discount_list = []                # Discount\n",
    "\n",
    "for i in range(0, 3):          # Running for loop with range to run this loop 2 times because each search returns 40 results\n",
    "\n",
    "    # Scrap Product Brand\n",
    "    brand_tags = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")              # Locating web element of Brand\n",
    "    for brand in brand_tags:                                                           # Iterating over web elements of Brand\n",
    "        brand_list.append(brand.text)                                                  # Appending each extracted text into empty list\n",
    "\n",
    "    # Scrap Product Description\n",
    "    description_tags = driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[1]\")   # Locating web element of Description\n",
    "    for description in description_tags:                                               # Iterating over web elements of Description\n",
    "        description_list.append(description.text)                                      # Appending each extracted text into empty list\n",
    "            \n",
    "    # Scrap Price\n",
    "    price_tags = driver.find_elements_by_xpath(\"//a[@class='_3bPFwb']/div[1]/div[1]\")  # Locating web elements of Price\n",
    "    for price in price_tags:                                                           # Iterating over web elements of Price\n",
    "        price_list.append(price.text)                                                  # Appending each extracted text into empty list\n",
    "\n",
    "    # Scrap Discount\n",
    "    discount_tags = driver.find_elements_by_xpath(\"//a[@class='_3bPFwb']/div/div[3]\")  # Locating web elements of Discount\n",
    "    for discount in discount_tags:                                                     # Iterating over web elements of Discount\n",
    "        discount_list.append(discount.text)                                            # Appending each extracted text into empty list\n",
    "\n",
    "    # Clicking on Next Button\n",
    "    if(i < 2):                              # Skip clicking Next Button last time because we only need top 100 results\n",
    "        driver.find_element_by_xpath(\".//*[contains(text(), 'Next')]\").click()          # Locating web element of next button and then clicking it\n",
    "    time.sleep(5)                           # Using time to pause the search engine for 5 second\n",
    "      \n",
    "# Display all data in 1 table\n",
    "products = pd.DataFrame({'Brand': brand_list, 'Product Description': description_list, 'Price': price_list})\n",
    "\n",
    "# Ticket-9158 - Skip Discount Percentage\n",
    "#products = pd.DataFrame({'Brand': brand_list, 'Product Description': description_list, 'Price': price_list, 'Discount': discount_list})\n",
    "\n",
    "products.index += 1                         # Start Index from 1\n",
    "\n",
    "# Keep only top 100 results\n",
    "products[0:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6035ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69cfd69",
   "metadata": {},
   "source": [
    "### Q 7. Go to the link - https://www.myntra.com/shoes. Set Price filter to “Rs. 7149 to Rs. 14099 ” , Color filter to “Black”. You have to scrape first 100 shoes data with following attributes:\n",
    "###### 1. Brand\n",
    "###### 2. Short Shoe Description\n",
    "###### 3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc0a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Opening up myntra.com website on automated chrome window\n",
    "driver.get(\"https://www.myntra.com/shoes\")\n",
    "\n",
    "# Entering Price filter as Rs. 7074 to Rs. 14049\n",
    "price_filter = driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label\")\n",
    "\n",
    "# Entering Color filter as Black\n",
    "color_filter = driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label\")\n",
    "\n",
    "color_filter.click()\n",
    "time.sleep(1)\n",
    "price_filter.click()\n",
    "time.sleep(1)\n",
    "\n",
    "brand_list = []                   # Shoe Brand\n",
    "description_list = []             # Shoe Short Description\n",
    "price_list = []                   # Shoe Price\n",
    "\n",
    "\n",
    "for i in range(0, 2):          # Running for loop with range to run this loop 2 times because each search returns 50 results\n",
    "\n",
    "    # Scrap Product Brand\n",
    "    brand_tags = driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")              # Locating web element of Brand\n",
    "    for brand in brand_tags:                                                                # Iterating over web elements of Brand\n",
    "        brand_list.append(brand.text)                                                       # Appending each extracted text into empty list\n",
    "\n",
    "    # Scrap Product Description\n",
    "    description_tags = driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "    for description in description_tags:                                                    # Iterating over web elements of Description\n",
    "        description_list.append(description.text)                                           # Appending each extracted text into empty list\n",
    "    \n",
    "    # Scrap Price\n",
    "    price_tags = driver.find_elements_by_xpath(\"//div[@class='product-price']/span[1]\")     # Locating web elements of Price\n",
    "    for price in price_tags:                                                                # Iterating over web elements of Price\n",
    "        price_list.append(price.text)                                                       # Appending each extracted text into empty list\n",
    "\n",
    "    # Clicking on Next Button\n",
    "    if(i < 1):\n",
    "        driver.find_element_by_xpath(\"//li[@class='pagination-next']/a\").click()  # Locating web element of next button and then clicking it\n",
    "    time.sleep(5)              # Using time to pause the search engine for 5 second\n",
    "\n",
    "# Display all data in 1 table\n",
    "products = pd.DataFrame({'Shoe Brand': brand_list, 'Short Shoe Description': description_list, 'Shoe Price': price_list})\n",
    "products.index += 1                                # Start Index from 1\n",
    "\n",
    "# Keep only top 100 results\n",
    "products[0:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa791b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d92c992",
   "metadata": {},
   "source": [
    "### Q 8. Go to webpage https://www.amazon.in/. Enter “Laptop” in the search field and then click the search icon. Then set CPU Type filter to “Intel Core i7” and “Intel Core i9”. You have to scrape first 10 laptops data with following attributes:\n",
    "###### 1. Title\n",
    "###### 2. Ratings\n",
    "###### 3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc24c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open amazon.com website on automated chrome window\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "# Finding element for Product Search Bar and Entering product Laptop\n",
    "search_field_product = driver.find_element_by_xpath(\"//input[@class='nav-input nav-progressive-attribute'][@type='text']\")  # Product Search Bar\n",
    "search_field_product.send_keys(\"Laptop\")\n",
    "\n",
    "# Clicking on Search Button\n",
    "search_button = driver.find_element_by_xpath(\"//input[@class='nav-input nav-progressive-attribute'][@type='submit']\")\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# Entering CPU Type Filter as 'Intel Core i7' and 'Intel Core i9'\n",
    "driver.find_element_by_xpath(\"//span[text()='Intel Core i7']\").click()\n",
    "time.sleep(5)\n",
    "# Ticket #9160  - Select any 1 CPU Type Filter\n",
    "# driver.find_element_by_xpath(\".//*[contains(text(), 'Intel Core i9')]\").click()\n",
    "# time.sleep(2)\n",
    "\n",
    "title_list = []                   # Laptop Title\n",
    "rating_list = []                  # Laptop Rating\n",
    "price_list = []                   # Laptop Price\n",
    "\n",
    "# Scrap Laptop Title\n",
    "title_tags = driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")        # Locating web element of Title\n",
    "for title in title_tags:                                                       # Iterating over web elements of Title\n",
    "    title_list.append(title.text)                                              # Appending each extracted text into empty list\n",
    "\n",
    "\n",
    "# Scrap Laptop Rating\n",
    "rating_tags = driver.find_elements_by_xpath(\"//a[@class='a-popover-trigger a-declarative']\")\n",
    "for rating in rating_tags:                                                     # Iterating over web elements of Rating\n",
    "    rating_list.append(rating.text)                                            # Appending each extracted text into empty list\n",
    "\n",
    "# Scrap Laptop Price\n",
    "price_tags = driver.find_elements_by_xpath(\"//span[@class='a-price']\")         # Locating web elements of Price\n",
    "for price in price_tags:                                                       # Iterating over web elements of Price\n",
    "    price_list.append(price.text)                                              # Appending each extracted text into empty list\n",
    "\n",
    "\n",
    "# Display all data in 1 table\n",
    "products = pd.DataFrame({'Title': title_list[0:10], 'Price': price_list[0:10]})     # Keep only top 10 results\n",
    "\n",
    "# Ticket-9160 - Skip Rating\n",
    "#products = pd.DataFrame({'Title': title_list[0:10], 'Ratings': rating_list[0:10], 'Price': price_list[0:10]})     # Keep only top 10 results\n",
    "\n",
    "products.index += 1                                # Start Index from 1\n",
    "products\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe5490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f801885a",
   "metadata": {},
   "source": [
    "### Q 9. Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida location from the webpage https://www.ambitionbox.com/. You have to scrape company name, No. of days ago when job was posted, Rating of the company. You have to scrape following attributes:\n",
    "###### 1. Company Name\n",
    "###### 2. No. of days ago when job was posted\n",
    "###### 3. Rating of the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c54c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open ambitionbox.com website on automated chrome window\n",
    "driver.get(\"https://www.ambitionbox.com/\")\n",
    "\n",
    "# Click 'Jobs' option in top menu\n",
    "driver.find_element_by_xpath(\"//a[@class='link jobs']\").click()\n",
    "time.sleep(1)\n",
    "\n",
    "# Finding element for Job Search Bar and Entering position Data Analyst\n",
    "search_field_designation = driver.find_element_by_xpath(\"//input[@class='input tt-input']\")      \n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "# Clicking on Search Button\n",
    "search_button = driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/button/span\")\n",
    "search_button.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# Click on 'Location' Tab\n",
    "driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[1]\").click()\n",
    "\n",
    "# Entering and selecting 'Noida' in Location Filter\n",
    "search_field_location = driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[2]/input\")\n",
    "search_field_location.send_keys(\"Noida\")\n",
    "driver.find_element_by_xpath(\"//input[@class='custom-radio'][@data='Noida']\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "# Scrape Company Name\n",
    "company_names = []\n",
    "company_tags = driver.find_elements_by_xpath(\"//p[@class='company body-medium']\")       # Locating web elements of Company Name\n",
    "for company_name in company_tags:                                                       # Iterating over web elements of Company Name\n",
    "    company_names.append(company_name.text)                                             # Appending each extracted text into empty list\n",
    "\n",
    "# Scrape Number of days since job was posted\n",
    "jobs_posted_since_list = []\n",
    "jobs_posted_since = driver.find_elements_by_xpath(\"//div[@class='other-info']//span[1]\")\n",
    "for job_posted in jobs_posted_since:\n",
    "    jobs_posted_since_list.append(job_posted.text)\n",
    "    \n",
    "# Scrape Company Rating\n",
    "company_ratings_list = []\n",
    "company_ratings_tags = driver.find_elements_by_xpath(\"//div[@class='info']//div//div//a[1]\")\n",
    "for company_rating in company_ratings_tags:\n",
    "    company_ratings_list.append(company_rating.get_attribute(\"title\"))\n",
    "\n",
    "# Display all data in 1 table\n",
    "jobs = pd.DataFrame({'Company Name': company_names, 'Jobs Posted Since': jobs_posted_since_list, 'Company Rating': company_ratings_list})\n",
    "jobs.index += 1                                # Start Index from 1\n",
    "jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b040d",
   "metadata": {},
   "source": [
    "### Q 10. Write a python program to scrape salary data for first 10 job results for Data Scientist Designation from the webpage https://www.ambitionbox.com/. You have to scrape company name, Number of salaries, Average salary, Minsalary, Max Salary. You have to scrape following attributes:\n",
    "###### 1. Company Name\n",
    "###### 2. Number of Salaries\n",
    "###### 3. Experience Required\n",
    "###### 4. Average Salary\n",
    "###### 5. Minimun Salary\n",
    "###### 6. Maximum Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a5908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First connect to the web driver\n",
    "driver = webdriver.Chrome(r\"/Users/nehamishra/MyPython/chromedriver\")\n",
    "\n",
    "# Open ambitionbox.com website on automated chrome window\n",
    "driver.get(\"https://www.ambitionbox.com/\")\n",
    "\n",
    "# Click 'Salaries' option in top menu\n",
    "driver.find_element_by_xpath(\"//a[@class='link salaries']\").click()\n",
    "time.sleep(1)\n",
    "\n",
    "# Finding element for Job Search Bar and Entering position Data Analyst\n",
    "search_field_designation = driver.find_element_by_xpath(\"//input[@class='tt-input']\")      \n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "time.sleep(1)\n",
    "driver.find_element_by_xpath(\"//div[@class='tt-dataset tt-dataset-job-profile-search']//div//div//div//p\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "# Scrape Company Name\n",
    "company_names = []\n",
    "company_tags = driver.find_elements_by_xpath(\"//div[@class='company-info']//div//a\")       # Locating web elements of Company Name\n",
    "for company_name in company_tags:                                                       # Iterating over web elements of Company Name\n",
    "    company_names.append(company_name.text)                                             # Appending each extracted text into empty list\n",
    "\n",
    "# Scrape Number of Salaries\n",
    "salaries_list = []\n",
    "salary_tags = driver.find_elements_by_xpath(\"//div[@class='company-info']//div[1]//span\")\n",
    "for salary in salary_tags:\n",
    "    salaries_list.append(salary.text)\n",
    "    \n",
    "# Scrape Experience Required\n",
    "experiences_list = []\n",
    "experience_tags = driver.find_elements_by_xpath(\"//div[@class='company-info']//div[2]\")\n",
    "for experience in experience_tags:\n",
    "    experiences_list.append(experience.text.split('\\n . \\n')[1])\n",
    "    \n",
    "# Scrape Average Salary\n",
    "average_salary_list = []\n",
    "average_salary_tags = driver.find_elements_by_xpath(\"//div[@class='average-indicator-wrapper']//p\")\n",
    "for avg_salary in average_salary_tags:\n",
    "    average_salary_list.append(avg_salary.text)\n",
    "\n",
    "# Scrape Minimum Salary\n",
    "minimum_salary_list = []\n",
    "minimum_salary_tags = driver.find_elements_by_xpath(\"//div[@class='salary-values']//div[1]\")\n",
    "for min_salary in minimum_salary_tags:\n",
    "    minimum_salary_list.append(min_salary.text)\n",
    "    \n",
    "# Scrape Maximum Salary\n",
    "maximum_salary_list = []\n",
    "maximum_salary_tags = driver.find_elements_by_xpath(\"//div[@class='salary-values']//div[2]\")\n",
    "for max_salary in maximum_salary_tags:\n",
    "    maximum_salary_list.append(max_salary.text)\n",
    "\n",
    "    \n",
    "# Display all data in 1 table\n",
    "jobs = pd.DataFrame({'Company Name': company_names, 'Number of Salaries': salaries_list, 'Experience Required': experiences_list[0:10], 'Average Salary': average_salary_list, 'Minimun Salary': minimum_salary_list, 'Maximum Salary': maximum_salary_list})\n",
    "jobs.index += 1                                # Start Index from 1\n",
    "jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95243515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1fe412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
