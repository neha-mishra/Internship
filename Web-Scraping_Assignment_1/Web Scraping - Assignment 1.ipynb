{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002813ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1c5127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802ca00",
   "metadata": {},
   "source": [
    "### Q 1. Write a python program to display all the header tags from wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b7447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHeaderTag(website_URL):\n",
    "    page = requests.get(website_URL)\n",
    "    \n",
    "    if(page.status_code == 200):\n",
    "        soup = BeautifulSoup(page.content)\n",
    "        header_containers = soup.find_all(\"span\", class_ = \"mw-headline\")\n",
    "        \n",
    "        headers = [] # empty list to store Header Tags\n",
    "\n",
    "        for header in header_containers:\n",
    "            headers.append(header.text)\n",
    "\n",
    "        # Making dataframe\n",
    "        df = pd.DataFrame({'Header List': headers})\n",
    "        df.index += 1\n",
    "        return(df)\n",
    "        \n",
    "    else:\n",
    "        print(\"Page Request Denied\")\n",
    "    \n",
    "getHeaderTag(\"https://en.wikipedia.org/wiki/Main_Page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7200748f",
   "metadata": {},
   "source": [
    "### Q 2. Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d1f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopMovies(website_URL):\n",
    "    \n",
    "    page = requests.get(website_URL)\n",
    "    \n",
    "    if(page.status_code == 200):\n",
    "        \n",
    "        soup = BeautifulSoup(page.content)\n",
    "        movie_containers = soup.find_all(\"div\", class_ = \"lister-item-content\")\n",
    "        \n",
    "        # Get Names of Movies\n",
    "        names = []\n",
    "        for movie in movie_containers:\n",
    "            names.append(movie.find(\"a\").text)\n",
    "        \n",
    "        # Get Rating of Movies\n",
    "        ratings = []\n",
    "        for movie in movie_containers:\n",
    "            ratings.append(movie.find(\"div\", class_ = \"inline-block ratings-imdb-rating\").text.replace(\"\\n\", \"\"))\n",
    "        \n",
    "        # Get Year of Release of Movies\n",
    "        yearOfRelease = []\n",
    "        #for movie in soup.find_all('span', class_ = \"lister-item-year text-muted unbold\").text.replace(\"(\", \"\").replace(\")\", \"\"):\n",
    "        for movie in movie_containers:\n",
    "            yearOfRelease.append(movie.find(\"span\", class_ = \"lister-item-year text-muted unbold\").text.replace(\"(\", \"\").replace(\")\", \"\"))\n",
    "        \n",
    "        # Making dataframe\n",
    "        df = pd.DataFrame({'Movie Name': names, 'Rating': ratings, 'Year Of Release': yearOfRelease})\n",
    "        df.index += 1\n",
    "        return(df)\n",
    "        \n",
    "    else:\n",
    "        print(\"Page Request Denied\")\n",
    "    \n",
    "getTopMovies(\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a946ad",
   "metadata": {},
   "source": [
    "### Q 3. Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopHindiMovies(website_URL):\n",
    "    \n",
    "    page = requests.get(website_URL)\n",
    "    \n",
    "    if(page.status_code == 200):\n",
    "        \n",
    "        soup = BeautifulSoup(page.content)\n",
    "        movie_containers = soup.find_all(\"div\", class_ = \"lister-item-content\")\n",
    "        \n",
    "        # Get Names of Movies\n",
    "        names = []\n",
    "        for movie in movie_containers:\n",
    "            names.append(movie.find(\"a\").text)\n",
    "        \n",
    "        # Get Rating of Movies\n",
    "        ratings = []\n",
    "        for movie in movie_containers:\n",
    "            ratings.append(movie.find(\"span\", class_ = \"ipl-rating-star__rating\").text.replace(\"\\n\", \"\"))\n",
    "        \n",
    "        # Get Year of Release of Movies\n",
    "        yearOfRelease = []\n",
    "        #for movie in soup.find_all('span', class_ = \"lister-item-year text-muted unbold\").text.replace(\"(\", \"\").replace(\")\", \"\"):\n",
    "        for movie in movie_containers:\n",
    "            yearOfRelease.append(movie.find(\"span\", class_ = \"lister-item-year text-muted unbold\").text.replace(\"(\", \"\").replace(\")\", \"\"))\n",
    "        \n",
    "        # Making dataframe\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame({'Movie Name': names, 'Rating': ratings, 'Year Of Release': yearOfRelease})\n",
    "        df.index += 1\n",
    "        return(df)\n",
    "        \n",
    "    else:\n",
    "        print(\"Page Request Denied\")\n",
    "    \n",
    "getTopHindiMovies(\"https://www.imdb.com/list/ls009997493/?sort=user_rating,desc&st_dt=&mode=detail&page=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3abd78",
   "metadata": {},
   "source": [
    "### Q 4. Write a python program to scrape product name, price and discounts from https://meesho.com/bagsladies/pl/p7vbp ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e16f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProductDetails(website_URL):\n",
    "    \n",
    "    page = requests.get(website_URL)\n",
    "    \n",
    "    if(page.status_code == 200):\n",
    "        \n",
    "        soup = BeautifulSoup(page.content)\n",
    "        product_containers = soup.find_all(\"div\", class_ = \"Card__BaseCard-sc-b3n78k-0 dUNFgg NewProductCard__DetailCard_Desktop-sc-j0e7tu-2 fqvaeS NewProductCard__DetailCard_Desktop-sc-j0e7tu-2 fqvaeS\")\n",
    "        \n",
    "        # Get Product Name\n",
    "        names = []\n",
    "        for product in product_containers:\n",
    "            names.append(product.find(\"p\").text)\n",
    "        \n",
    "        # Get Product Price\n",
    "        prices = []\n",
    "        for product in product_containers:\n",
    "            prices.append(product.find(\"h5\").text)\n",
    "        \n",
    "        # Get Product Discount\n",
    "        discounts = []\n",
    "        for product in product_containers:\n",
    "            discounts.append(product.find(\"span\", class_ = \"Text__StyledText-sc-oo0kvp-0 lnonyH\").text)\n",
    "        \n",
    "        # Making dataframe\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame({'Product Name': names, 'Price': prices, 'Discounts': discounts})\n",
    "        df.index += 1\n",
    "        return(df)\n",
    "    \n",
    "    else:\n",
    "        print(\"Page Request Denied\")\n",
    "        \n",
    "getProductDetails(\"https://meesho.com/bagsladies/pl/p7vbp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10151c3",
   "metadata": {},
   "source": [
    "### Q 5. Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "    a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "    b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "    c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a6bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "    \n",
    "def getTop_ODI_Teams(website_URL, gender):\n",
    "    \n",
    "    page = requests.get(website_URL)\n",
    "    \n",
    "    if(page.status_code == 200):\n",
    "        \n",
    "        soup = BeautifulSoup(page.content)\n",
    "        team_containers = soup.find_all(\"tr\")\n",
    "        \n",
    "        # Remove top title/header row\n",
    "        team_containers.pop(0)\n",
    "        \n",
    "        # Keep only top 10 records\n",
    "        del team_containers[10:]\n",
    "        \n",
    "        # Get Team Name\n",
    "        names = []\n",
    "        for team in team_containers:\n",
    "            names.append(team.find(\"span\", class_ = \"u-hide-phablet\").text)\n",
    "        \n",
    "        # Get Team Matches Record\n",
    "        matches = []\n",
    "        for team in team_containers:\n",
    "            \n",
    "            if(team.find(\"td\", class_ = \"rankings-block__banner--matches\")):\n",
    "                matches.append(team.find(\"td\", class_ = \"rankings-block__banner--matches\").text)\n",
    "                \n",
    "            if(team.find(\"td\", class_ = \"table-body__cell u-center-text\")):\n",
    "                matches.append(team.find(\"td\", class_ = \"table-body__cell u-center-text\").text)\n",
    "            \n",
    "        # Get Team Points Record\n",
    "        points = []\n",
    "        for team in team_containers:\n",
    "            \n",
    "            if(team.find(\"td\", class_ = \"rankings-block__banner--points\")):\n",
    "                points.append(team.find(\"td\", class_ = \"rankings-block__banner--points\").text)\n",
    "                \n",
    "            if(team.find(\"td\", class_ = \"table-body__cell u-center-text\")):\n",
    "                points.append(team.find_all(\"td\", class_ = \"table-body__cell u-center-text\")[1].text)\n",
    "            \n",
    "        # Get Team Ratings Record\n",
    "        ratings = []\n",
    "        for team in team_containers:\n",
    "            \n",
    "            if(team.find(\"td\", class_ = \"rankings-block__banner--rating u-text-right\")):\n",
    "                ratings.append(team.find(\"td\", class_ = \"rankings-block__banner--rating u-text-right\").text.replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "                \n",
    "            if(team.find(\"td\", class_ = \"table-body__cell u-text-right rating\")):\n",
    "                ratings.append(team.find(\"td\", class_ = \"table-body__cell u-text-right rating\").text.replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "        \n",
    "        print(color.BOLD + color.UNDERLINE + \"Top 10 ODI Teams in \" + gender + \"’s Cricket\" + color.END)\n",
    "        # Making dataframe\n",
    "        df = pd.DataFrame({'Team': names, 'Matches': matches, 'Points': points, 'Rating': ratings})\n",
    "        df.index += 1\n",
    "        return(df)\n",
    "    \n",
    "    else:\n",
    "        print(\"Page Request Denied\")\n",
    "        \n",
    "def getTop_ODI_Players(website_URL, gender, player_Type):\n",
    "    \n",
    "    page = requests.get(website_URL)\n",
    "    \n",
    "    if(page.status_code == 200):\n",
    "        \n",
    "        soup = BeautifulSoup(page.content)\n",
    "        \n",
    "        team_containers = soup.find_all(\"tr\", class_ = \"table-body\")\n",
    "        team_containers.append(soup.find(\"tr\", class_ = \"rankings-block__banner\"))\n",
    "        \n",
    "        # Move last row (for number #1 ranking) to top\n",
    "        team_containers.insert(0, team_containers.pop())\n",
    "        \n",
    "        # Keep only top 10 records\n",
    "        del team_containers[10:]\n",
    "        \n",
    "        # Get Player Name\n",
    "        names = []\n",
    "        for player in team_containers:\n",
    "            \n",
    "            if(player.find(\"div\", class_ = \"rankings-block__banner--name-large\")):\n",
    "                names.append(player.find(\"div\", class_ = \"rankings-block__banner--name-large\").text)\n",
    "            else:\n",
    "                names.append(player.find(\"a\").text)\n",
    "            \n",
    "        # Get Player Team\n",
    "        teams = []\n",
    "        for player in team_containers:\n",
    "            \n",
    "            if(player.find(\"div\", class_ = \"rankings-block__banner--nationality\")):\n",
    "                teams.append(player.find(\"div\", class_ = \"rankings-block__banner--nationality\").text.replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "            else:\n",
    "                teams.append(player.find(\"span\", class_ = \"table-body__logo-text\").text)\n",
    "            \n",
    "        #Get Player Rating\n",
    "        ratings = []\n",
    "        for player in team_containers:\n",
    "            \n",
    "            if(player.find(\"div\", class_ = \"rankings-block__banner--rating\")):\n",
    "                ratings.append(player.find(\"div\", class_ = \"rankings-block__banner--rating\").text.replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "            else:\n",
    "                ratings.append(player.find(\"td\", class_ = \"table-body__cell rating\").text)\n",
    "            \n",
    "        print(color.BOLD + color.UNDERLINE + \"Top 10 ODI \" + player_Type + \" in \" + gender + \"’s Cricket\" + color.END)\n",
    "        # Making dataframe\n",
    "        df = pd.DataFrame({'Player': names, 'Team': teams, 'Rating': ratings})\n",
    "        df.index += 1\n",
    "        return(df)\n",
    "        \n",
    "    else:\n",
    "        print(\"Page Request Denied\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 5.a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "getTop_ODI_Teams(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\", \"Men\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a62448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 5.b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "getTop_ODI_Players(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\", \"Men\", \"Batsmen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 5.c) Top 10 ODI bowlers along with the records of their team and rating.\n",
    "getTop_ODI_Players(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\", \"Men\", \"Bowlers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685acee5",
   "metadata": {},
   "source": [
    "### Q 6. Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "    a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "    b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "    c) Top 10 women’s ODI all-rounder along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 6.a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "getTop_ODI_Teams(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\", \"Women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 6.b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "getTop_ODI_Players(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\", \"Women\", \"Batsmen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 6.c) Top 10 women’s ODI all-rounder along with the records of their team and rating\n",
    "getTop_ODI_Players(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\", \"Women\", \"All-Rounders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3a6e0",
   "metadata": {},
   "source": [
    "### Q 7. Write a python program to scrape details of all the posts from coreyms.com. Scrape the heading, date, content and the code for the video from the link for the youtube video from the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ec532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPostDetails(website_URL):\n",
    "    \n",
    "    page = requests.get(website_URL)\n",
    "    \n",
    "    if(page.status_code == 200):\n",
    "        \n",
    "        soup = BeautifulSoup(page.content)\n",
    "        post_containers = soup.find_all(\"article\")\n",
    "        \n",
    "        post_heading = []\n",
    "        post_date = []\n",
    "        post_content = []\n",
    "        post_video_link = []\n",
    "        \n",
    "        for post in post_containers:\n",
    "            post_heading.append(post.find(\"a\", class_ = \"entry-title-link\").text)\n",
    "            post_date.append(post.find(\"time\", class_ = \"entry-time\").text)\n",
    "            post_content.append(post.find(\"div\", class_ = \"entry-content\").text.replace('\\n', ''))\n",
    "            if(post.find(\"iframe\")):\n",
    "                post_video_link.append(post.find(\"iframe\", class_ = \"youtube-player\")['src'])\n",
    "            else:\n",
    "                post_video_link.append(\"\")\n",
    "        \n",
    "        # Making dataframe\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame({'Heading': post_heading, 'Date': post_date, 'Content': post_content, 'Video Link': post_video_link})\n",
    "        df.index += 1\n",
    "        return(df)\n",
    "    \n",
    "    else:\n",
    "        print(\"Page Request Denied\")\n",
    "        \n",
    "getPostDetails(\"https://coreyms.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22acfd2",
   "metadata": {},
   "source": [
    "### Q 8. Write a python program to scrape house details from mentioned URL. It should include house title, location, area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar, Rajaji Nagar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f976fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHouseDetails(website_URL):\n",
    "    \n",
    "    page = requests.get(website_URL)\n",
    "    \n",
    "    if(page.status_code == 200):\n",
    "        \n",
    "        soup = BeautifulSoup(page.content)\n",
    "        house_containers = soup.find_all(\"div\", class_ = \"bg-white rounded-2 bg-clip-padding overflow-hidden my-1.2p mx-0.5p tp:border-b-0 tp:shadow-cardShadow tp:mt-0.5p tp:mx-0 tp:mb:1p hover:cursor-pointer nb__2_XSE\")\n",
    "        \n",
    "        #print(len(house_containers))\n",
    "        \n",
    "        house_title = []\n",
    "        location = []\n",
    "        house_EMI = []\n",
    "        house_price = []\n",
    "        \n",
    "        for house in house_containers:\n",
    "            house_title.append(house.find(\"span\", class_ = \"overflow-hidden overflow-ellipsis whitespace-nowrap max-w-80pe po:max-w-full\").text)\n",
    "            location.append(house.find(\"div\", class_ = \"mt-0.5p overflow-hidden overflow-ellipsis whitespace-nowrap max-w-70 text-gray-light leading-4 po:mb-0 po:max-w-95\").text)\n",
    "            #house_EMI.append(house.find(\"div\", class_ = \"font-semi-bold text-14\").text\n",
    "            #house_price.append(house.find(\"meta\", content_ = \"Minimum Deposit\").text)\n",
    "            \n",
    "#             if(post.find(\"iframe\")):\n",
    "#             else:\n",
    "#                 post_video_link.append(\"\")\n",
    "        \n",
    "        # Making dataframe\n",
    "        import pandas as pd\n",
    "        #df = pd.DataFrame({'House Title': house_title, 'Location/Area': location, 'EMI': house_EMI, 'Price': house_price})\n",
    "        df = pd.DataFrame({'House Title': house_title, 'Location/Area': location})\n",
    "        \n",
    "        df.index += 1\n",
    "        return(df)\n",
    "    \n",
    "    else:\n",
    "        print(\"Page Request Denied\")\n",
    "        \n",
    "getHouseDetails(\"https://www.nobroker.in/property/sale/bangalore/multiple?searchParam=W3sibGF0IjoxMi45NzgzNjkyLCJsb24iOjc3LjY0MDgzNTYsInBsYWNlSWQiOiJDaElKa1FOM0dLUVdyanNSTmhCUUpyaEdEN1UiLCJwbGFjZU5hbWUiOiJJbmRpcmFuYWdhciIsInNob3dNYXAiOmZhbHNlfSx7ImxhdCI6MTIuOTMwNzczNSwibG9uIjo3Ny41ODM4MzAyLCJwbGFjZUlkIjoiQ2hJSjJkZGxaNWdWcmpzUmgxQk9BYWYtb3JzIiwicGxhY2VOYW1lIjoiSmF5YW5hZ2FyIiwic2hvd01hcCI6ZmFsc2V9LHsibGF0IjoxMi45OTgxNzMyLCJsb24iOjc3LjU1MzA0NDU5OTk5OTk5LCJwbGFjZUlkIjoiQ2hJSnhmVzREUE05cmpzUktzTlRHLTVwX1FRIiwicGxhY2VOYW1lIjoiUmFqYWppbmFnYXIiLCJzaG93TWFwIjpmYWxzZX1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4ff95",
   "metadata": {},
   "source": [
    "### Q 9. Write a python program to scrape mentioned details from dineout.co.in :\n",
    "\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80537e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHouseDetails(website_URL):\n",
    "    \n",
    "    page = requests.get(website_URL)\n",
    "    \n",
    "    if(page.status_code == 200):\n",
    "        \n",
    "        soup = BeautifulSoup(page.content)\n",
    "        restaurant_containers = soup.find_all(\"div\", class_ = \"restnt-main-wrap clearfix\")\n",
    "        \n",
    "        #print(restaurant_containers)\n",
    "        \n",
    "        restaurant_name = []\n",
    "        cuisines = []\n",
    "        location = []\n",
    "        ratings = []\n",
    "        image_URL = []\n",
    "        \n",
    "        \n",
    "        for restaurant in restaurant_containers:\n",
    "            restaurant_name.append(restaurant.find(\"a\", class_ = \"restnt-name ellipsis\").text)\n",
    "            location.append(restaurant.find(\"div\", class_ = \"restnt-loc ellipsis\").text)\n",
    "            ratings.append(restaurant.find(\"div\", class_ = \"restnt-rating rating-4\").text)\n",
    "            \n",
    "            cuisine = restaurant.find(\"span\", class_ = \"double-line-ellipsis\")\n",
    "            c = \"\"\n",
    "            for cu in cuisine.find_all(\"a\"):\n",
    "                c = c + cu.text + \", \"\n",
    "            cuisines.append(c[ : -2])\n",
    "            \n",
    "            if(restaurant.find(\"img\", class_ = \"no-img\")):\n",
    "                image_URL.append(restaurant.find(\"img\", class_ = \"no-img\")['data-src'])\n",
    "            else:\n",
    "                image_URL.append(\"\")\n",
    "        \n",
    "        # Making dataframe\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame({'Restaurant Name': restaurant_name, 'Cuisine': cuisines, 'Location': location, 'Ratings': ratings, 'Image URL': image_URL})\n",
    "        \n",
    "        df.index += 1\n",
    "        return(df)\n",
    "    \n",
    "    else:\n",
    "        print(\"Page Request Denied\")\n",
    "        \n",
    "getHouseDetails(\"https://www.dineout.co.in/delhi-restaurants/welcome-back\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28770e71",
   "metadata": {},
   "source": [
    "### Q 10. Write a python program to scrape first 10 product details which include product name , price , Image URL from https://www.bewakoof.com/women-tshirts?ga_q=tshirts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba91aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTop10ProductDetails(website_URL):\n",
    "    \n",
    "    page = requests.get(website_URL)\n",
    "    \n",
    "    if(page.status_code == 200):\n",
    "        \n",
    "        soup = BeautifulSoup(page.content)\n",
    "        product_containers = soup.find_all(\"div\", class_ = \"plp-product-card\")\n",
    "        del product_containers[10:]\n",
    "        \n",
    "        names = []\n",
    "        prices = []\n",
    "        image_URL = []\n",
    "        \n",
    "        for product in product_containers:\n",
    "            names.append(product.find(\"h3\").text)\n",
    "            \n",
    "            if(product.find(\"span\", class_ = \"discountedPriceText\")):\n",
    "                prices.append(product.find(\"span\", class_ = \"discountedPriceText\").text)\n",
    "            elif(product.find(\"span\", class_ = \"actualPriceText\")):\n",
    "                prices.append(product.find(\"span\", class_ = \"actualPriceText\").text)\n",
    "            else:\n",
    "                prices.append(\"-\")\n",
    "            \n",
    "            image_URL.append(product.find(\"img\", class_ = \"productImgTag\")['src'])\n",
    "            \n",
    "        # Making dataframe\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame({'Product Name': names, 'Price': prices, 'Image URL': image_URL})\n",
    "        df.index += 1\n",
    "        return(df)\n",
    "    \n",
    "    else:\n",
    "        print(\"Page Request Denied\")\n",
    "        \n",
    "getTop10ProductDetails(\"https://www.bewakoof.com/women-t-shirts?sort=new\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
